#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π
–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞—Ç–∫–∏ –º—É—Å–æ—Ä–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è
"""

import re
from typing import List, Dict, Set
from collections import defaultdict

def normalize_company_name(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    if not name:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–µ
    name = re.sub(r'^\s*\d+\.\s*', '', name)
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
    prefixes = [
        r'^–ö–æ–º–ø–∞–Ω–∏—è:?\s*',
        r'^–ù–∞–∑–≤–∞–Ω–∏–µ:?\s*', 
        r'^–∫–æ–º–ø–∞–Ω–∏—è:?\s*',
        r'^–Ω–∞–∑–≤–∞–Ω–∏–µ:?\s*',
        r'^–ö–æ–º–ø–∞–Ω–∏:?\s*'
    ]
    for prefix in prefixes:
        name = re.sub(prefix, '', name, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö –∏ –ø–æ—Å–ª–µ —Ç–∏—Ä–µ
    name = re.sub(r'\s*\([^)]*\)\s*$', '', name)
    name = re.sub(r'\s*\[[^\]]*\]\s*$', '', name)
    name = re.sub(r'\s*[-‚Äì‚Äî]\s*.*$', '', name)
    name = re.sub(r'\s*,\s*.*$', '', name)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    name = ' '.join(name.split())
    
    return name.strip()

def get_canonical_name(names: List[str]) -> str:
    """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    if not names:
        return ""
    
    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ
    names = [n for n in names if n.strip()]
    if not names:
        return ""
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    def score_name(name: str) -> tuple:
        # –ß–µ–º –º–µ–Ω—å—à–µ —Å–∫–æ—Ä, —Ç–µ–º –ª—É—á—à–µ
        length = len(name)
        has_quotes = '"' in name or "'" in name
        has_brackets = '(' in name or '[' in name
        has_dash = '‚Äì' in name or '‚Äî' in name or ' - ' in name
        has_comma = ',' in name
        has_hash = '#' in name
        is_caps = name.isupper()
        
        return (has_quotes, has_brackets, has_dash, has_comma, has_hash, is_caps, length)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    sorted_names = sorted(names, key=score_name)
    return sorted_names[0]

def is_still_garbage(name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º—É—Å–æ—Ä–æ–º –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
    if not name or len(name.strip()) < 2:
        return True
    
    name_lower = name.lower().strip()
    
    # –û—Å—Ç–∞—Ç–∫–∏ –∫–æ–¥–∞
    code_fragments = [
        'function', 'import', 'export', 'const', 'console.log',
        'then(', '=>', '```', 'useeffect', 'usestate', 'return',
        'typeof', 'async', 'await', 'promise', '.map', '.filter',
        'react', 'component', 'props', 'state'
    ]
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã
    tech_phrases = [
        '—Ç–µ—Ö —Å–æ–±–µ—Å', '—Ç–µ—Ö—Å–æ–±–µ—Å', '–ª–∞–π–≤–∫–æ–¥–∏–Ω–≥', '—Å–∫—Ä–∏–Ω–∏–Ω–≥',
        '—ç—Ç–∞–ø', '—Ñ–∏–Ω–∞–ª', '–∏–Ω—Ç–µ—Ä–≤—å—é', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ',
        '–∑–∞–¥–∞—á–∞', '–≤–æ–ø—Ä–æ—Å', '—Ä–µ—à–µ–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º'
    ]
    
    # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    descriptive = [
        '–∞—É—Ç—Å—Ç–∞—Ñ—Ñ', '–∞—É—Ç—Å–æ—Ä—Å', '–ø—Ä–æ–µ–∫—Ç', '–∫–æ–º–∞–Ω–¥–∞', '—Å—Ç—Ä–∏–º',
        '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–æ—Ç–¥–µ–ª', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '–≥—Ä—É–ø–ø–∞'
    ]
    
    # –î–∞—Ç—ã –∏ —á–∏—Å–ª–∞
    if re.match(r'^\d+[\.\-/]\d+', name) or re.match(r'^\d+$', name):
        return True
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä
    for fragment in code_fragments + tech_phrases:
        if fragment in name_lower:
            return True
    
    # –ï—Å–ª–∏ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    words = name_lower.split()
    if len(words) <= 2 and all(word in descriptive for word in words):
        return True
    
    # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è)
    if len(name) > 80:
        return True
    
    # –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã –±–µ–∑ –±—É–∫–≤
    if not re.search(r'[–∞-—è—ëa-z]', name_lower):
        return True
    
    return False

def group_similar_companies(companies: List[str]) -> Dict[str, List[str]]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π"""
    groups = defaultdict(list)
    
    for company in companies:
        normalized = normalize_company_name(company)
        if not normalized or is_still_garbage(normalized):
            continue
            
        # –ö–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ - —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        key = re.sub(r'[^–∞-—è—ëa-z0-9]', '', normalized.lower())
        
        # –û—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–∞–Ω–∫–æ–≤/–∫–æ–º–ø–∞–Ω–∏–π
        if '—Å–±–µ—Ä' in key:
            if '–±–∞–Ω–∫' in key or key == '—Å–±–µ—Ä':
                key = '—Å–±–µ—Ä'
            else:
                pass  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –°–±–µ—Ä–¢–µ—Ö, –°–±–µ—Ä–ú–∞—Ä–∫–µ—Ç –∏ —Ç.–¥.
        elif '–∞–ª—å—Ñ–∞' in key and '–±–∞–Ω–∫' in key:
            key = '–∞–ª—å—Ñ–∞–±–∞–Ω–∫'
        elif key in ['—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '—Ç–±–∞–Ω–∫']:
            key = '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ'
        elif key in ['–º—Ç—Å']:
            key = '–º—Ç—Å'
        elif key in ['–≤—Ç–±']:
            key = '–≤—Ç–±'
        elif '—è–Ω–¥–µ–∫—Å' in key:
            key = '—è–Ω–¥–µ–∫—Å'
        
        groups[key].append(company)
    
    return groups

def final_clean_companies():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π"""
    
    print("üßπ –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –°–ü–ò–°–ö–ê –ö–û–ú–ü–ê–ù–ò–ô")
    print("=" * 60)
    
    # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫
    try:
        with open('clean_companies_list.txt', 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª clean_companies_list.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Å–µ–∫—Ü–∏–∏ "–ß–ò–°–¢–´–ô –°–ü–ò–°–û–ö –ö–û–ú–ü–ê–ù–ò–ô"
    lines = content.split('\n')
    companies = []
    in_companies_section = False
    
    for line in lines:
        line = line.strip()
        if '–ß–ò–°–¢–´–ô –°–ü–ò–°–û–ö –ö–û–ú–ü–ê–ù–ò–ô:' in line:
            in_companies_section = True
            continue
        elif '–£–î–ê–õ–ï–ù–ù–´–ô –ú–£–°–û–†:' in line:
            break
        elif in_companies_section and line:
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
            clean_line = re.sub(r'^\s*\d+\.\s*', '', line)
            if clean_line:
                companies.append(clean_line)
    
    print(f"üìã –ò—Å—Ö–æ–¥–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies)}")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
    groups = group_similar_companies(companies)
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
    final_companies = []
    duplicates_removed = 0
    garbage_removed = 0
    
    for key, group in groups.items():
        if len(group) > 1:
            duplicates_removed += len(group) - 1
            print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç—ã '{key}': {group}")
        
        canonical = get_canonical_name(group)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä
        normalized = normalize_company_name(canonical)
        if not is_still_garbage(normalized) and normalized:
            final_companies.append(normalized)
        else:
            garbage_removed += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    final_companies.sort(key=lambda x: x.lower())
    
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {len(final_companies)}")
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}")
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –º—É—Å–æ—Ä–∞: {garbage_removed}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = 'final_companies_list.txt'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("–§–ò–ù–ê–õ–¨–ù–´–ô –°–ü–ò–°–û–ö –ö–û–ú–ü–ê–ù–ò–ô\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"–í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(final_companies)}\n")
        f.write(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}\n") 
        f.write(f"–£–¥–∞–ª–µ–Ω–æ –º—É—Å–æ—Ä–∞: {garbage_removed}\n\n")
        f.write("–°–ü–ò–°–û–ö –ö–û–ú–ü–ê–ù–ò–ô:\n")
        f.write("-" * 30 + "\n")
        
        for i, company in enumerate(final_companies, 1):
            f.write(f"{i:3d}. {company}\n")
    
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_file}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¢–û–ü-20
    print(f"\nüìä –ü–ï–†–í–´–ï 20 –ö–û–ú–ü–ê–ù–ò–ô:")
    for i, company in enumerate(final_companies[:20], 1):
        print(f"{i:3d}. {company}")

if __name__ == "__main__":
    final_clean_companies() 