import re
import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..' )))

from app.database import get_db
from app.models import ContentBlock

def normalize_title_v1(title: str) -> str:
    """–¢–µ–∫—É—â–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—Å–ª–∏—à–∫–æ–º –≥—Ä—É–±–∞—è)"""
    title = re.sub(r'\[.*?\]\(.*?\)', '', title)
    return re.sub(r'[^\w\s]', '', title.lower().strip())

def normalize_title_v2(title: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è"""
    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ markdown [text](url)
    title = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', title)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    title = re.sub(r'\s+', ' ', title.lower().strip())
    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–Ω—É–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ
    title = re.sub(r'[#\-\*\+]', '', title)
    return title.strip()

def extract_companies_from_md_text(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    companies = []
    patterns = [
        r'–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤\s*[-\s]*([^\n]+)',
        r'–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤\s*[-\s]*([^\n]+)', 
        r'–≤—Å—Ç—Ä–µ—á–∞–ª—Å—è –≤\s*[-\s]*([^\n]+)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            clean_companies = re.split(r'[,\n\-]', match.strip())
            for company in clean_companies:
                company = company.strip().lower()
                if company and len(company) > 2:
                    companies.append(company)
    
    return list(set(companies))

def parse_md_files_improved():
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ .md —Ñ–∞–π–ª–æ–≤"""
    tasks = []
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    
    folders = [
        os.path.join(base_dir, 'js', '–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏'), 
        os.path.join(base_dir, 'js', '—Ä–µ—à–µ–Ω–∏—è'), 
        os.path.join(base_dir, 'react'), 
        os.path.join(base_dir, 'ts', '–ó–∞–¥–∞—á–∏')
    ]
    
    for folder_path in folders:
        if not os.path.exists(folder_path):
            continue
            
        for root, _, files in os.walk(folder_path):
            for file_name in files:
                if file_name.endswith('.md'):
                    file_path = os.path.join(root, file_name)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏: #### Title –∏–ª–∏ ## Title –∏ —Ç.–¥.
                        lines = content.split('\n')
                        current_title = None
                        current_content = ""
                        
                        for line in lines:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
                            if header_match:
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–¥–∞—á—É –µ—Å–ª–∏ –µ—Å—Ç—å
                                if current_title and current_content.strip():
                                    companies = extract_companies_from_md_text(current_content)
                                    if companies:  # –¢–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏
                                        tasks.append({
                                            'title': current_title,
                                            'content': current_content.strip(),
                                            'companies': companies,
                                            'file': file_name,
                                            'normalized_v1': normalize_title_v1(current_title),
                                            'normalized_v2': normalize_title_v2(current_title)
                                        })
                                
                                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
                                current_title = header_match.group(2).strip()
                                current_content = ""
                            else:
                                current_content += line + "\n"
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–¥–∞—á—É
                        if current_title and current_content.strip():
                            companies = extract_companies_from_md_text(current_content)
                            if companies:
                                tasks.append({
                                    'title': current_title,
                                    'content': current_content.strip(),
                                    'companies': companies,
                                    'file': file_name,
                                    'normalized_v1': normalize_title_v1(current_title),
                                    'normalized_v2': normalize_title_v2(current_title)
                                })
                                
                    except Exception as e:
                        print(f"Error reading file {file_path}: {e}")
    
    return tasks

def analyze_matching():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è"""
    print("=== –ê–ù–ê–õ–ò–ó –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø ===\n")
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
    db = next(get_db())
    db_blocks = db.query(ContentBlock).all()
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ .md —Ñ–∞–π–ª–æ–≤
    md_tasks = parse_md_files_improved()
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ ContentBlocks –≤ –ë–î: {len(db_blocks)}")
    print(f"   ‚Ä¢ –ó–∞–¥–∞—á–∏ —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏ –≤ .md: {len(md_tasks)}")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    db_blocks_v1 = {normalize_title_v1(block.blockTitle): block for block in db_blocks}
    db_blocks_v2 = {normalize_title_v2(block.blockTitle): block for block in db_blocks}
    
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ –ë–î:")
    for i, block in enumerate(db_blocks[:10]):
        print(f"   {i+1}. '{block.blockTitle}' -> v1: '{normalize_title_v1(block.blockTitle)}' -> v2: '{normalize_title_v2(block.blockTitle)}'")
    
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ .md (—Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏):")
    for i, task in enumerate(md_tasks[:10]):
        print(f"   {i+1}. '{task['title']}' -> v1: '{task['normalized_v1']}' -> v2: '{task['normalized_v2']}' -> –∫–æ–º–ø–∞–Ω–∏–∏: {task['companies']}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    matches_v1 = 0
    matches_v2 = 0
    partial_matches = []
    
    print(f"\nüìà –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
    
    for md_task in md_tasks:
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ v1
        if md_task['normalized_v1'] in db_blocks_v1:
            matches_v1 += 1
            
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ v2  
        if md_task['normalized_v2'] in db_blocks_v2:
            matches_v2 += 1
            
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for db_title_norm, db_block in db_blocks_v2.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –æ–¥–Ω–æ –≤ –¥—Ä—É–≥–æ–º
            if (md_task['normalized_v2'] in db_title_norm or 
                db_title_norm in md_task['normalized_v2']) and len(db_title_norm) > 3:
                partial_matches.append({
                    'md_title': md_task['title'],
                    'db_title': db_block.blockTitle,
                    'companies': md_task['companies']
                })
                break
    
    print(f"   ‚Ä¢ –°–æ–≤–ø–∞–¥–µ–Ω–∏–π v1 (—Å—Ç–∞—Ä–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è): {matches_v1}")
    print(f"   ‚Ä¢ –°–æ–≤–ø–∞–¥–µ–Ω–∏–π v2 (–Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è): {matches_v2}")
    print(f"   ‚Ä¢ –ß–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(partial_matches)}")
    
    if partial_matches:
        print(f"\nüéØ –ü—Ä–∏–º–µ—Ä—ã —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
        for i, match in enumerate(partial_matches[:10]):
            print(f"   {i+1}. MD: '{match['md_title']}' <-> DB: '{match['db_title']}' -> {match['companies']}")
    
    # –ü–æ–∏—Å–∫ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö
    unmatched_md = []
    for md_task in md_tasks:
        found = False
        for db_title_norm in db_blocks_v2.keys():
            if (md_task['normalized_v2'] == db_title_norm or
                md_task['normalized_v2'] in db_title_norm or 
                db_title_norm in md_task['normalized_v2']) and len(db_title_norm) > 3:
                found = True
                break
        if not found:
            unmatched_md.append(md_task)
    
    print(f"\n‚ùå –ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–∑ .md ({len(unmatched_md)}):")
    for i, task in enumerate(unmatched_md[:15]):
        print(f"   {i+1}. '{task['title']}' -> {task['companies']}")
    
    db.close()

if __name__ == "__main__":
    analyze_matching() 