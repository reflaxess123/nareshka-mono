import re
import os
import json
from collections import defaultdict

def extract_all_from_md_file(file_path: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏ –∑–∞–¥–∞—á –∏–∑ .md —Ñ–∞–π–ª–∞"""
    if not os.path.exists(file_path):
        return {}
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    result = {
        'file_path': file_path,
        'tasks': [],
        'companies_in_blocks': [],
        'companies_in_headers': [],
        'all_headers': [],
        'company_blocks_raw': [],
        'statistics': {}
    }
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–ª—é–±–æ–≥–æ —É—Ä–æ–≤–Ω—è)
    all_headers = re.findall(r'^(#{1,6})\s*(.+)$', content, re.MULTILINE)
    for level, header_text in all_headers:
        result['all_headers'].append({
            'level': len(level),
            'text': header_text.strip(),
            'normalized': header_text.strip().lower()
        })
    
    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–ª–æ–∫–∏ "–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤" —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    company_block_patterns = [
        r'–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤.*?(?=\n\S|\n#+|\n```|\Z)',
        r'–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤.*?(?=\n\S|\n#+|\n```|\Z)', 
        r'–≤—Å—Ç—Ä–µ—á–∞–ª—Å—è –≤.*?(?=\n\S|\n#+|\n```|\Z)',
        r'–ø–æ–ø–∞–¥–∞–ª–æ—Å—å –≤.*?(?=\n\S|\n#+|\n```|\Z)'
    ]
    
    for pattern in company_block_patterns:
        blocks = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
        for block in blocks:
            result['company_blocks_raw'].append(block)
            
            # –ü–∞—Ä—Å–∏–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –±–ª–æ–∫–∞
            lines = block.split('\n')
            companies_in_block = []
            
            for line in lines[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–ø–∏—Å–∫–æ–≤
                cleaned_line = re.sub(r'^[\t\s]*[-*+]\s*', '', line).strip()
                cleaned_line = re.sub(r'^[\t\s]*\d+\.\s*', '', cleaned_line).strip()
                
                if cleaned_line and len(cleaned_line) > 1:
                    companies_in_block.append(cleaned_line)
            
            result['companies_in_blocks'].extend(companies_in_block)
    
    # 3. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–¥–∞—á
    sections = re.split(r'(^#{1,6}\s+.+)$', content, flags=re.MULTILINE)
    
    current_task = None
    current_content = ""
    
    for section in sections:
        if re.match(r'^#{1,6}\s+', section):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–¥–∞—á—É
            if current_task and current_content:
                task_companies = []
                
                # –ò—â–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –∑–∞–¥–∞—á–∏
                for pattern in company_block_patterns:
                    task_blocks = re.findall(pattern, current_content, re.IGNORECASE | re.DOTALL)
                    for block in task_blocks:
                        lines = block.split('\n')
                        for line in lines[1:]:
                            cleaned = re.sub(r'^[\t\s]*[-*+]\s*', '', line).strip()
                            if cleaned and len(cleaned) > 1:
                                task_companies.append(cleaned)
                
                result['tasks'].append({
                    'title': current_task,
                    'normalized_title': re.sub(r'^#+\s*', '', current_task).strip().lower(),
                    'companies': task_companies,
                    'content_length': len(current_content),
                    'has_code': '```' in current_content
                })
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
            current_task = section.strip()
            current_content = ""
        else:
            current_content += section
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–¥–∞—á—É
    if current_task and current_content:
        task_companies = []
        for pattern in company_block_patterns:
            task_blocks = re.findall(pattern, current_content, re.IGNORECASE | re.DOTALL)
            for block in task_blocks:
                lines = block.split('\n')
                for line in lines[1:]:
                    cleaned = re.sub(r'^[\t\s]*[-*+]\s*', '', line).strip()
                    if cleaned and len(cleaned) > 1:
                        task_companies.append(cleaned)
        
        result['tasks'].append({
            'title': current_task,
            'normalized_title': re.sub(r'^#+\s*', '', current_task).strip().lower(),
            'companies': task_companies,
            'content_length': len(current_content),
            'has_code': '```' in current_content
        })
    
    # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∫–æ–º–ø–∞–Ω–∏–π
    potential_company_headers = []
    for header in result['all_headers']:
        header_text = header['normalized']
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —É—Ä–æ–≤–Ω—è 1-3, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–º–ø–∞–Ω–∏—è–º–∏
        if header['level'] <= 3 and len(header_text) < 50:
            # –ò—Å–∫–ª—é—á–∞–µ–º —è–≤–Ω–æ —Ç–µ—Ö–Ω–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not re.match(r'^(–∑–∞–¥–∞—á|example|–ø—Ä–∏–º–µ—Ä|counter|todo|—Å–ø–∏—Å–∫–∏|—Ä–µ—Ñ–∞–∫—Ç–æ—Ä|—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ|other|use|test|—Ä–µ—à–µ–Ω–∏)', header_text):
                potential_company_headers.append(header_text)
    
    result['companies_in_headers'] = potential_company_headers
    
    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    result['statistics'] = {
        'total_headers': len(result['all_headers']),
        'total_tasks': len(result['tasks']),
        'tasks_with_companies': len([t for t in result['tasks'] if t['companies']]),
        'total_company_mentions': len(result['companies_in_blocks']),
        'potential_company_headers': len(potential_company_headers),
        'company_blocks_found': len(result['company_blocks_raw'])
    }
    
    return result

def analyze_all_md_files():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ .md —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ .md —Ñ–∞–π–ª—ã
    md_files = []
    for root, dirs, files in os.walk('..'):
        for file in files:
            if file.endswith('.md'):
                md_files.append(os.path.join(root, file))
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ .md —Ñ–∞–π–ª–æ–≤: {len(md_files)}")
    print("=" * 80)
    
    all_results = []
    all_companies = set()
    all_tasks = []
    
    for file_path in md_files:
        print(f"\nüìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {file_path}")
        result = extract_all_from_md_file(file_path)
        
        if result['tasks']:
            all_results.append(result)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            for company in result['companies_in_blocks']:
                all_companies.add(company.lower().strip())
            
            for company in result['companies_in_headers']:
                all_companies.add(company.lower().strip())
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
            all_tasks.extend(result['tasks'])
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª—É
            stats = result['statistics']
            print(f"   üìä –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {stats['total_headers']}")
            print(f"   üìã –ó–∞–¥–∞—á: {stats['total_tasks']}")
            print(f"   üè¢ –ó–∞–¥–∞—á —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏: {stats['tasks_with_companies']}")
            print(f"   üîó –£–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π: {stats['total_company_mentions']}")
            print(f"   üè™ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {stats['potential_company_headers']}")
    
    print("\n" + "=" * 80)
    print("üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_results)}")
    print(f"   üìã –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(all_tasks)}")
    print(f"   üè¢ –ó–∞–¥–∞—á —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏: {len([t for t in all_tasks if t['companies']])}")
    print(f"   üè™ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {len(all_companies)}")
    
    print(f"\nüè¢ –í–°–ï –ù–ê–ô–î–ï–ù–ù–´–ï –ö–û–ú–ü–ê–ù–ò–ò ({len(all_companies)}):")
    sorted_companies = sorted(list(all_companies))
    for i, company in enumerate(sorted_companies, 1):
        print(f"   {i:3d}. {company}")
    
    print(f"\nüìã –ü–†–ò–ú–ï–†–´ –ó–ê–î–ê–ß –° –ö–û–ú–ü–ê–ù–ò–Ø–ú–ò:")
    tasks_with_companies = [t for t in all_tasks if t['companies']][:20]  # –ü–µ—Ä–≤—ã–µ 20
    for i, task in enumerate(tasks_with_companies, 1):
        companies_str = ', '.join(task['companies'][:3])  # –ü–µ—Ä–≤—ã–µ 3 –∫–æ–º–ø–∞–Ω–∏–∏
        if len(task['companies']) > 3:
            companies_str += f" (–∏ –µ—â–µ {len(task['companies']) - 3})"
        print(f"   {i:2d}. '{task['normalized_title']}' -> [{companies_str}]")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    output_data = {
        'summary': {
            'total_files': len(all_results),
            'total_tasks': len(all_tasks),
            'tasks_with_companies': len([t for t in all_tasks if t['companies']]),
            'unique_companies': len(all_companies)
        },
        'all_companies': sorted(list(all_companies)),
        'all_tasks': all_tasks,
        'file_results': all_results
    }
    
    with open('extraction_results.json', 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'extraction_results.json'")
    
    return output_data

if __name__ == "__main__":
    analyze_all_md_files() 