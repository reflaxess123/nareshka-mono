import re
import os
import json
from collections import defaultdict

# –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
KNOWN_COMPANIES = {
    '—è–Ω–¥–µ–∫—Å', '—Å–±–µ—Ä', '—Ç–∏–Ω–∫–æ—Ñ—Ñ', '—Ç–∏–Ω–∫–æ—Ñ', '–ª–µ–º–º–∞', 'wb', '–∏–Ω–Ω–æ—Ç–µ—Ö', '–æ—Ç–ø', '–æ—Ç–ø –±–∞–Ω–∫', 
    '–≥–∞–∑–ø—Ä–æ–º', '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫', 'profsoft', 'altenar', 'it-one', '—Å–±–µ—Ä—Ç–µ—Ö', '–∞–ª—å—Ñ–∞', 
    '–∞–ª—å—Ñ–∞–±–∞–Ω–∫', '–∞–ª—å—Ñ–∞-–±–∞–Ω–∫', '–≤–∫', '–∞–≤—Ç–æ–º–∞–∫–æ–Ω', '–º–æ–π—Å–∫–ª–∞–¥', '—Ü–µ–∑–∏–æ', 'qugo', 'ibs', 
    '—Ä–æ—Å–±–∞–Ω–∫', '–æ–æ–æ –º–æ–±–∞–π–ª–¥–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç', '—Å–∏–±—É—Ä', '–≥–∫ —Ç1', '–±–∏–ª–∞–π–Ω', 'kotelov', '–≤—Ç–±',
    'unisender', '–æ–∑–æ–Ω', '–ø–æ–ª–µ.—Ä—Ñ', 'artw', '–∞–≤–∏—Ç–æ', 'realweb', 'itfb', '–∫–∏–±–µ—Ä–ª–∞–±', 
    'portalbilet', 'marpla', '—Ä–æ—Å–≥–æ—Å—Å—Ç—Ä–∞—Ö', 'right line', 'quantum art', '–∞–ª–∞—Å–∫–∞—Ä —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
    '—Å–µ–ª–µ–∫—Ç–∏', '–ø—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫', '—Å–±–µ—Ä–¥–µ–≤–∞–π—Å—ã', 'funbox', '—Ü—É–º', 'it baltic', '—Å–±–µ—Ä–∫–æ—Ä—É—Å',
    '—Ä—É—Ç—É–±', '–µ–≤—Ä–æ—Ç–µ—Ö–∫–ª–∏–º–∞—Ç', 'luxsoft', 'tilda', '–∞–Ω—Ç–∞—Ä–∞', '—Ñ–∏–Ω–∞–º', '–±–∞–ª–∞–Ω—Å –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞',
    '–ª–∞–± –∫–∞—Å–ø', '—è–Ω–¥–µ–∫—Å —Ç–∞–∫—Å–∏', 'click2money', 'kts', 'premium it solution', 'moex',
    'eesee', '–¥–æ–º.—Ä—Ñ', '–ª–∏–≥–∞ —Ü–∏—Ñ—Ä–æ–≤–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏', '—è–Ω–¥–µ–∫—Å –ø—Ä–æ', 'coding team', '—é—Ç—ç–∏—Ä',
    'goinvest', 'yandex.pay', 'yandex.multitrack', '–±–∞—É–º', '–∫–æ–∫–æ—Å.group', '—Å–∫–≤–∞–¥',
    '—Ç–æ—á–∫–∞ –±–∞–Ω–∫', 'vision', 'sbertech', '–æ–Ω–ª–∞–π–Ω —à–∫–æ–ª–∞ —Ç–µ—Ç—Ä–∏–∫–∞', '—Å—Ñ–µ—Ä–∞', 'yandex',
    'mail.ru', 'kaspersky', '—Ä–∞–π—Ñ–∞–π–∑–µ–Ω', 'tele2', '—Ç–µ–ª–µ2', '—Ä—Å—Ö–±', '—Ä—Å—Ö–±-–∏–Ω—Ç–µ—Ö',
    '—Ç–µ—Ö–∑–æ—Ä', '—É—Ä–±–∞–Ω—Ç–µ—Ö', 'datsteam', 'dats team', 'unybrands', '–º—Ç—Å', 'northflank',
    '–∫—å—é–≥–æ', '–∫–æ–¥–∏–Ω–≥ —Ç–∏–º', '—Å—Ç—Ä–∏–º —Ç–µ–ª–µ–∫–æ–º', 'stream telecom', 'realtime',
    '–∫–ª–∏–∫—Ç—É–º–∞–Ω–∏', 'click to money', '—Ö–æ–ª–¥–∏–Ω–≥', '–±—Ñ—Ç',
    'ivi', '—Ä–∞—Å—á–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è', '—Å–±–µ—Ä –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤', '–ø–æ—á—Ç–∞ —Ä–æ—Å—Å–∏–∏', '—Å–æ–≤–∫–æ–º–±–∞–Ω–∫',
    '—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å', '–∫–∞—Å–ø–µ—Ä—Å–∫–∏–π', '–Ω–µ—Ç–æ–ª–æ–≥–∏—è', 'skillbox', 'geekbrains', 'hexlet',
    '–∫–æ–Ω—Ç—É—Ä', '—Ç–µ–ª–µ2', 'csssr', 'dogma', 'severstal', 'tbq', 'loalty labs', 'r vision',
    'heavyfunc', '–≥–æ—Å–Ω–∏–∏–∞—Å', '–∫–∏–±–µ—Ä —Ä–æ–º', '–¥–æ–º–∫–ª–∏–∫', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –¥–æ–≤–µ—Ä–∏—è'
}

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π
COMPANY_NORMALIZATIONS = {
    'c–±–µ—Ä–∫–æ—Ä—É—Å': '—Å–±–µ—Ä–∫–æ—Ä—É—Å',
    'it one': 'it-one', 
    'it baltic': 'it-baltic',
    '–∞–ª—å—Ñ–∞ –±–∞–Ω–∫': '–∞–ª—å—Ñ–∞-–±–∞–Ω–∫',
    '–∞–ª—å—Ñ–∞–±–∞–Ω–∫': '–∞–ª—å—Ñ–∞-–±–∞–Ω–∫',
    'click2money': 'click to money',
    'datsteam': 'dats team',
    'dats team': 'datsteam',
    'stream telecom': '—Å—Ç—Ä–∏–º —Ç–µ–ª–µ–∫–æ–º',
    '–≤–±': 'wb',
    '–≥–∞–∑–ø—Ä–æ–º': '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫',
    '—Ç–∏–Ω–∫–æ—Ñ': '—Ç–∏–Ω–∫–æ—Ñ—Ñ',
    'yandex': '—è–Ω–¥–µ–∫—Å',
    '—Ç–µ–ª–µ2': 'tele2',
    '–∫–∞—Å–ø–µ—Ä—Å–∫–∏–π': 'kaspersky',
    '—Å–±–µ—Ä—Ç–µ—Ö': 'sbertech',
    '–º—Ç—Å': 'mts',
    'loalty labs': 'loyalty labs',
    '–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ': '–≤–∫'
}

# –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –º—É—Å–æ—Ä - —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ù–ò–ö–û–ì–î–ê –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è–º–∏
ABSOLUTE_GARBAGE = {
    '—Ç–æ –µ—Å—Ç—å', '–Ω–µ–ª—å–∑—è', '–±–µ–∑', '–Ω–∞–ø–∏—Å–∞—Ç—å', '—á–∞—Å—Ç–æ', '—Ä–µ—à–µ–Ω–æ', '—Ä–∞–∑', '–≤ —Ü–µ–ª–æ–º', '–ø—Ä–∏–º–µ—Ä', 
    '–∑–∞–¥–∞—á–∞', '–∑–∞–¥–∞—á–∏', '—Ä–µ—à–µ–Ω–∏–µ', '—Ä–µ—à–µ–Ω–∏—è', '–∫–∞–∫', '–Ω–∞–¥–æ', '–Ω—É–∂–Ω–æ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∏–µ', 
    '–æ–ø–∏—Å–∞–Ω–∏–µ', '—Å–º—ã—Å–ª', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '–∏–ª–∏', '–ø—Ä–æ–≤–µ—Ä–∫–∞', '–æ—à–∏–±–∫–∞', '–æ—à–∏–±–∫–∏', 
    '—Ç–µ—Å—Ç', '—Ç–µ—Å—Ç—ã', '—à–∞–±–ª–æ–Ω', '—à–∞–±–ª–æ–Ω—ã', '–æ–±—ä–µ–∫—Ç', '–º–∞—Å—Å–∏–≤', '—Å—Ç—Ä–æ–∫–∞', '—á–∏—Å–ª–æ', '—Ñ—É–Ω–∫—Ü–∏—è', 
    '–∫–ª–∞—Å—Å—ã', '—Ö—É–∫–∏', '–ø–æ–ª—è', '—Ç–∏–ø—ã', '–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å', '–ø—Ä–æ–º–∏—Å—ã', '–∫–æ–ª–±—ç–∫–∏', '–∑–∞–º—ã–∫–∞–Ω–∏—è', 
    '—Ä–µ–∫—É—Ä—Å–∏—è', '–∞–ª–≥–æ—Ä–∏—Ç–º—ã', '—Å—Ç—Ä—É–∫—Ç—É—Ä—ã', '–¥–∞–Ω–Ω—ã–µ', '—Å–µ—Ä–≤–µ—Ä', '–∫–ª–∏–µ–Ω—Ç', '—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥', '–±—ç–∫–µ–Ω–¥', 
    'api', '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', '–±–¥', '–æ–æ–ø', '–ø–∞—Ç—Ç–µ—Ä–Ω—ã', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', '–¥–∏–∑–∞–π–Ω', '—Å–∏—Å—Ç–µ–º–∞', 
    '–∫–æ–º–∞–Ω–¥–∞', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '–ø—Ä–∞–∫—Ç–∏–∫–∞', '—Ç–µ–æ—Ä–∏—è', '—Å—Ç–∞—Ç—å—è', '–≥–ª–∞–≤–∞', 
    '—á–∞—Å—Ç—å', '–ø—Ä–æ—Å—Ç–æ', '—Å–ª–æ–∂–Ω–æ', '–¥–ª—è', '–æ—Ç', '—Å', '–∏–∑', '–≤', '–Ω–∞', '–ø–æ', '–∑–∞', '–ø—Ä–∏', 
    '–¥–æ', '–ø–æ—Å–ª–µ', '—Å–æ', '–ø—É—Ç—å', '–∫–ª—é—á', '–∑–Ω–∞—á–µ–Ω–∏–µ', '–∫–æ–¥', '–∫–æ–¥–∞', '—Ç–µ–∫—Å—Ç', '—Ç–µ–∫—Å—Ç–∞', 
    '–∫–æ–º–ø–æ–Ω–µ–Ω—Ç', '–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã', '–≤–∏–¥–∂–µ—Ç', '–≤–∏–¥–∂–µ—Ç—ã', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞', '—Å—Ç—Ä–∞–Ω–∏—Ü—ã', '–º–æ–¥–µ–ª—å', '–º–æ–¥–µ–ª–∏', 
    '—Å–ª–∞–π—Å', '—Å–ª–∞–π—Å—ã', '—Ö—É–∫', '—Å—Ç–∏–ª–∏', '–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã', '—É—Ç–∏–ª–∏—Ç—ã', '–∫–æ–Ω—Ç–µ–∫—Å—Ç', '—Ä–æ—É—Ç–µ—Ä', '–ø—Ä–æ–≤–∞–π–¥–µ—Ä', 
    '—Ä–µ–¥–∞–∫—Å', '–Ω–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏–µ–π', '—Å–∫–∞–∑–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å', '—Å—á–µ—Ç—á–∏–∫ –ø–æ–ª—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '—Å—á–µ—Ç—á–∏–∫–∏', 
    'state', '–±–∞—Ç—á–∏–Ω–≥', '–±–µ–∑ set', 'platform v ui kits', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ o(1)', 
    '–Ω–µ–ª—å–∑—è –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–∞—Å—Å–∏–≤', '—Ç–æ –µ—Å—Ç—å inplace', '–Ω–∞ –ª–∏—á–Ω–æ–º —Å–æ–±–µ—Å–µ', '–Ω–µ –º—É—Ç–∏—Ä—É–µ–º', 
    '–¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤', '—á–∞—Å—Ç–æ –≤ —Ü–µ–ª–æ–º', 'todo', 'list',
    '–ø—Å–±', '–≥—Ä—É–ø–ø', 'auto tech',
    '—Å–≤–∏—Å—Ç—É–Ω–æ–≤–∞ –µ–∫–∞—Ç–µ—Ä–∏–Ω–∞ –∞–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–Ω–∞', '—á–µ—Ä–µ–∑ –Ω–µ—Ç–±–µ–ª–ª –∞–∫—Ä –ª–∞–±—Å',
    'hard –∑–∞–¥–∞—á–∏', 'key remapping –≤ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —Ç–∏–ø–∞—Ö', '—Ç–æ –µ—Å—Ç—å inplace)',
    '–≤–∫ canonizepath –≤ —Ç—Ä–µ—Ö —á–∞—Å—Ç—è—Ö', '–∫–∏–±–µ—Ä —Ä–æ–º',
    '—è–Ω–¥–µ–∫—Å –ø—Ä–æ )' # –ï—Å–ª–∏ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º—É—Å–æ—Ä, –∞ –Ω–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏
}

# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è–º–∏
TECH_KEYWORDS = {
    # –§—É–Ω–∫—Ü–∏–∏ –∏ –º–µ—Ç–æ–¥—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ—à–µ–Ω–∏–π
    'withretry', 'parallel', 'fetchdata', 'fetchdatawithretry', 'url limit', 'parallellimit', 
    'parallelrequest', 'requestbus', 'add two promises', 'timelimit', 'promisify',
    'withtimeout', 'fetchdatawithdelay', '–∫–∞—Å—Ç–æ–º–Ω—ã–π race', '–∫–∞—Å—Ç–æ–º–Ω—ã–π finally', '–∫–∞—Å—Ç–æ–º–Ω—ã–π any',
    '–∫–∞—Å—Ç–æ–º–Ω—ã–π allsettled', '–∫–∞—Å—Ç–æ–º–Ω—ã–π all',
    
    # TypeScript —É—Ç–∏–ª–∏—Ç—ã –∏ —Ç–∏–ø—ã
    'callback', 'concat', 'exclude', 'exclude + omit', 'formfields', 'getdetails', 'getinfo', 
    'getinfo 2', 'getproperty 1', 'getproperty 2', 'getproperty 3', 'join', 'last', 'merge', 
    'myawaited', 'omit', 'parameters', 'partial', 'pluck', 'readonly', 'template literal types',
    '–∫–∞—Å—Ç–æ–º–Ω—ã–µ utility', '—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–µ —É—Å–ª–æ–≤–Ω—ã–µ —Ç–∏–ø—ã', '—Å–æ–∑–¥–∞–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã—Ö —Ç–∏–ø–æ–≤',
    'types', 'myasyncfunction',
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
    'mymath', 'printfiles', 'scroll&random', 'execution', '–ø–∞—á–∫–∞ –º–∏–Ω–∏ –∑–∞–¥–∞—á',
    
    # –ù–æ–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏/–∑–∞–¥–∞—á–∏ (–ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ—à–µ–Ω–∏–π)
    '14 withretry', '15 parallel', '16 fetchdata', '17 fetchdatawithretry', '18 url limit',
    '19 parallellimit', '20 parallelrequest', '21 requestbus',
    
    # –û–±—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    'function', 'return', 'const', 'var', 'let', 'class', 'interface', 'type', 'import', 'export',
    'async', 'await', 'promise', 'then', 'catch', 'finally', 'resolve', 'reject', 'timeout',
    'fetch', 'response', 'request', 'api', 'json', 'xml', 'html', 'css', 'js', 'ts', 'jsx', 'tsx',
    'react', 'vue', 'angular', 'component', 'hook', 'state', 'props', 'context', 'provider',
    'reducer', 'action', 'store', 'selector', 'middleware', 'thunk', 'saga', 'effect',
    'nodejs', 'npm', 'yarn', 'webpack', 'vite', 'babel', 'eslint', 'prettier', 'typescript',
    'javascript', 'python', 'java', 'golang', 'rust', 'php', 'ruby', 'swift', 'kotlin',
    'database', 'sql', 'nosql', 'mongodb', 'postgresql', 'mysql', 'redis', 'kafka',
    'docker', 'kubernetes', 'nginx', 'apache', 'linux', 'windows', 'macos', 'git', 'github',
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
    'singleton', 'factory', 'observer', 'strategy', 'decorator', 'facade', 'adapter',
    'bridge', 'composite', 'flyweight', 'proxy', 'chain', 'command', 'iterator',
    'mediator', 'memento', 'prototype', 'state', 'template', 'visitor',
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    'array', 'object', 'string', 'number', 'boolean', 'null', 'undefined', 'symbol',
    'map', 'set', 'weakmap', 'weakset', 'array buffer', 'typed array', 'data view',
    'stack', 'queue', 'linked list', 'tree', 'graph', 'hash table', 'heap',
    'sort', 'search', 'binary search', 'linear search', 'bubble sort', 'quick sort',
    'merge sort', 'heap sort', 'insertion sort', 'selection sort',
    
    # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã (–Ω–µ –∫–æ–º–ø–∞–Ω–∏–∏)
    '—Ü–µ–ø–æ—á–∫–∞ –∏–∑ 3 –ø—Ä–æ–º–∏—Å–æ–≤', '–≤—ã–≤–æ–¥–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º',
    '–æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ —Ñ–µ—Ç—á–µ–º', '–∫–∞—Å—Ç–æ–º–Ω—ã–π race', '–∫–∞—Å—Ç–æ–º–Ω—ã–π finally',
}

def aggressive_clean_company_name(company: str) -> str:
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç –≤—Å–µ–≥–æ –º—É—Å–æ—Ä–∞"""
    if not company:
        return ""
    
    original = company
    company = company.lower().strip()
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –≤ —Å–∫–æ–±–∫–∞—Ö, –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö –∏ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
    company = re.sub(r'\([^)]*\)', '', company) # –£–¥–∞–ª—è–µ–º –∫—Ä—É–≥–ª—ã–µ —Å–∫–æ–±–∫–∏
    company = re.sub(r'\[[^\]]*\]', '', company) # –£–¥–∞–ª—è–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏
    company = re.sub(r'\{[^}]*\}', '', company) # –£–¥–∞–ª—è–µ–º —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏

    # –£–¥–∞–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã-–º—É—Å–æ—Ä
    garbage_phrases = [
        '–Ω–µ–ª—å–∑—è –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–∞—Å—Å–∏–≤, —Ç–æ –µ—Å—Ç—å inplace',
        '–±–µ–∑ set, —Å–∫–∞–∑–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –±—É–¥–µ—Ç –ª–∏ set —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ–±—ä–µ–∫—Ç–∞–º–∏',
        'platform v ui kits',
        '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ o\\(1\\)',
        '–¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤',
        '—á–µ—Ä–µ–∑ –Ω–µ—Ç–±–µ–ª–ª –∞–∫—Ä –ª–∞–±—Å',
        '–Ω–∞ –ª–∏—á–Ω–æ–º —Å–æ–±–µ—Å–µ',
        '–Ω–µ –º—É—Ç–∏—Ä—É–µ–º',
        '—á–∞—Å—Ç–æ –≤ —Ü–µ–ª–æ–º',
        '—Å–≤–∏—Å—Ç—É–Ω–æ–≤–∞ –µ–∫–∞—Ç–µ—Ä–∏–Ω–∞ –∞–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–Ω–∞',
        '–ø—Å–±',
        '–≥—Ä—É–ø–ø',
        'auto tech',
        '\\+ —Ç–∏–ø–∏–∑–∞—Ü–∏—è',
        '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞', # –ü–µ—Ä–µ–Ω–µ—Å —Å—é–¥–∞
        '—Ö–æ–ª–¥–∏–Ω–≥', # –ü–µ—Ä–µ–Ω–µ—Å —Å—é–¥–∞
        '–∏–ø' # –ü–µ—Ä–µ–Ω–µ—Å —Å—é–¥–∞
    ]
    
    for phrase in garbage_phrases:
        company = re.sub(phrase, '', company, flags=re.IGNORECASE)
    
    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä –≤ –Ω–∞—á–∞–ª–µ (—Ç–∏–ø–∞ "14. ")
    company = re.sub(r'^\d+\.\s*', '', company) # –£–¥–∞–ª—è–µ–º "14. "
    company = re.sub(r'[-‚Äì‚Äî_./,:;]+', ' ', company) # –î–æ–±–∞–≤–∏–ª : –∏ ; –¥–ª—è –ª—É—á—à–µ–π –æ—á–∏—Å—Ç–∫–∏
    company = re.sub(r'\s+', ' ', company)
    company = company.strip()
    
    return company

def normalize_company_name(company: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏"""
    clean_name = aggressive_clean_company_name(company)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    if clean_name in COMPANY_NORMALIZATIONS:
        return COMPANY_NORMALIZATIONS[clean_name]
    
    return clean_name

def is_definitely_company(company: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–º–ø–∞–Ω–∏–∏"""
    if not company or len(company.strip()) < 2:
        return False
    
    clean_name = aggressive_clean_company_name(company)
    
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ, —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –≤ —Å–∫–æ–±–∫–∞—Ö –ø–æ—Å–ª–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ (–¥–ª—è –¥–æ–ø. —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏)
    clean_name = re.sub(r'\([^)]*\)', '', clean_name)
    clean_name = re.sub(r'\[[^\]]*\]', '', clean_name)
    clean_name = re.sub(r'\{[^}]*\}', '', clean_name)
    clean_name = clean_name.strip()

    if not clean_name:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –±—É–∫–≤—É
    if not re.search(r'[–∞-—èa-zA-Z]', clean_name):
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –º—É—Å–æ—Ä (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    if clean_name in ABSOLUTE_GARBAGE:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if clean_name in TECH_KEYWORDS:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ —á–∞—Å—Ç—å —Å–ª–æ–≤–∞ —è–≤–ª—è–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –º—É—Å–æ—Ä–æ–º (–¥–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Å–ª–æ–≤)
    for tech_word in TECH_KEYWORDS:
        if len(tech_word) > 3 and tech_word in clean_name and tech_word != clean_name:
            return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞ –∏–∑ ABSOLUTE_GARBAGE
    for garbage in ABSOLUTE_GARBAGE:
        if clean_name == garbage or (len(garbage) > 4 and garbage in clean_name and garbage != clean_name):
            return False

    # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏, –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
    if len(clean_name) < 3 and clean_name not in KNOWN_COMPANIES and not re.match(r'^[a-zA-Z]{2,3}$', clean_name):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
    for known in KNOWN_COMPANIES:
        if clean_name == known or (len(known) > 2 and known in clean_name) or (len(clean_name) > 2 and clean_name in known):
            return True
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏
    # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∫–æ–º–ø–∞–Ω–∏–π
    company_indicators = [
        r'\b(–±–∞–Ω–∫|—Ç–µ—Ö|—Å–æ—Ñ—Ç|–ª–∞–±|–≥—Ä—É–ø–ø?|—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏|—Ä–µ—à–µ–Ω–∏—è|–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞|—Å–∏—Å—Ç–µ–º—ã|—Å–µ—Ä–≤–∏—Å|—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞|–∞–π—Ç–∏)\b',
        r'\b(–∫–æ–º–ø–∞–Ω–∏|–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è|—Ö–æ–ª–¥–∏–Ω–≥|–∞–æ|–æ–æ–æ|–ø–∫–∫|–Ω–∏–∏|–∫–±|–∑–∞–≤–æ–¥|—Ñ–∞–±—Ä–∏–∫–∞|—Ü–µ–Ω—Ç—Ä|–∏–Ω—Å—Ç–∏—Ç—É—Ç|—Å—Ç—É–¥–∏—è|–º–æ–±–∞–π–ª–¥–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç)\b',
        r'\b(–∞–≤—Ç–æ|–¥–µ–≤|—Ñ–∏–Ω|–ø—Ä–æ|–ø–ª—é—Å|–Ω–µ—Ç|–≥–µ–π–º—Å|–º–µ–¥–∏–∞|–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ|–∏–Ω—Ñ–æ|–¥–∞—Ç–∞|–¥–∏–¥–∂–∏—Ç–∞–ª|–ø–∞—Ä—Ç–Ω–µ—Ä|–≥—Ä—É–ø–ø–∞|–æ–Ω–ª–∞–π–Ω —à–∫–æ–ª–∞|–∏–Ω—Ç–µ—Ö)\b'
    ]
    
    for pattern in company_indicators:
        if re.search(pattern, clean_name, re.IGNORECASE):
            return True
    
    # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã + –±—É–∫–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, t1, wb), –Ω–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –Ω–æ–º–µ—Ä–æ–º
    if re.search(r'\d', clean_name) and re.search(r'[–∞-—èa-zA-Z]', clean_name) and not clean_name.isdigit():
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –≤–µ—Ä—Å–∏—é –∏–ª–∏ –Ω–æ–º–µ—Ä –∑–∞–¥–∞—á–∏
        if not re.fullmatch(r'\d+(\.\d+){0,2}', clean_name) and \
           not re.fullmatch(r'v\d+(\.\d+){0,2}', clean_name) and \
           not re.fullmatch(r'task\s*\d+', clean_name) and \
           not re.fullmatch(r'\d+\s+\w+', clean_name):  # –∏—Å–∫–ª—é—á–∞–µ–º "14 withretry"
            return True
    
    # –ï—Å–ª–∏ —ç—Ç–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –±—É–∫–≤ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ) –∏ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ
    latin_chars = len(re.findall(r'[a-zA-Z]', clean_name))
    cyrillic_chars = len(re.findall(r'[–∞-—è–ê-–Ø]', clean_name))
    
    if latin_chars > 2 and latin_chars >= cyrillic_chars / 2: # –ë–æ–ª–µ–µ –≥–∏–±–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        # –ò—Å–∫–ª—é—á–∞–µ–º, –µ—Å–ª–∏ —ç—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –æ–±—ã—á–Ω–æ–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω
        # –£–±—Ä–∞–ª –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–≥–æ–≤, –æ—Å—Ç–∞–≤–∏–ª —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        if clean_name.lower() not in [w.lower() for w in TECH_KEYWORDS] and \
           not re.match(r'^(a|the|in|on|at|is|or|as|if|to|by|up|out|off|on)$', clean_name.lower()):
            return True
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å–æ—Å—Ç–∞–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤) –∏ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ñ—Ä–∞–∑–∞
    words = clean_name.split()
    if len(words) >= 2 and all(len(word) >= 2 for word in words):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ñ—Ä–∞–∑–∞ –∏–∑ TECH_KEYWORDS –∏–ª–∏ ABSOLUTE_GARBAGE
        if not any(phrase in clean_name for phrase in TECH_KEYWORDS | ABSOLUTE_GARBAGE):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
            if not any(word in ['–∑–∞–¥–∞—á–∞', '–ø—Ä–∏–º–µ—Ä', '—Ä–µ—à–µ–Ω–∏–µ', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '—Ñ—É–Ω–∫—Ü–∏—è', '–º–µ—Ç–æ–¥', '—Å–ø–∏—Å–æ–∫', '—Å–±–æ—Ä–Ω–∏–∫'] for word in words):
                return True
    
    return False

def extract_companies_from_block(block_text: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –±–ª–æ–∫–∞ '–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤' —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    companies = []
    lines = block_text.split('\n')
    
    for line in lines[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å "–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤"
        # –£–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–æ–≤
        cleaned_line = re.sub(r'^[\t\s]*[-*+‚Ä¢]\s*', '', line).strip()
        cleaned_line = re.sub(r'^[\t\s]*\d+\.\s*', '', cleaned_line).strip()
        
        if not cleaned_line:
            continue
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º, —Ç–æ—á–∫–∞–º —Å –∑–∞–ø—è—Ç–æ–π
        potential_companies = re.split(r'[,;]', cleaned_line)
        
        for company in potential_companies:
            company = company.strip()
            if is_definitely_company(company):
                normalized = normalize_company_name(company)
                if normalized and normalized not in companies:
                    companies.append(normalized)
    
    return companies

def extract_companies_from_headers(headers: list, file_path: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–ª—è —Ñ–∞–π–ª–æ–≤ —Ç–∏–ø–∞ –†–µ—Ä–µ–Ω–¥–µ—Ä.md) —Å —É—á–µ—Ç–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    companies = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ñ–∞–π–ª–æ–º —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (—Ä–µ—à–µ–Ω–∏—è/—Ä–µ—Ñ–∞–∫—Ç–æ—Ä/—Ä–µ—Ä–µ–Ω–¥–µ—Ä)
    is_company_header_file = any(keyword in file_path.lower() 
                                for keyword in ['—Ä–µ—Ä–µ–Ω–¥–µ—Ä', '—Ä–µ—Ñ–∞–∫—Ç–æ—Ä', '—Ä–µ—à–µ–Ω–∏—è'])
    
    for header in headers:
        header_text = header['text'].strip()
        
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ (–∫–∞–∫ –≤ —Ñ–∞–π–ª–∞—Ö —Å —Ä–µ—à–µ–Ω–∏—è–º–∏)
        if re.match(r'^\d+\.\s*', header_text):
            continue

        # –î–ª—è —Ñ–∞–π–ª–æ–≤, –≥–¥–µ –æ–∂–∏–¥–∞—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (—É—Ä–æ–≤–µ–Ω—å <=3, –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ, –Ω–µ –æ–±—â–∏–π –º—É—Å–æ—Ä)
        if (is_company_header_file and 
            header['level'] <= 3 and 
            len(header_text) < 50 and
            not re.match(r'^(–∑–∞–¥–∞—á|example|–ø—Ä–∏–º–µ—Ä|counter|todo|—Å–ø–∏—Å–∫–∏|—Ä–µ—Ñ–∞–∫—Ç–æ—Ä|—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ|other|use|test|—Ä–µ—à–µ–Ω–∏|–º–∏–Ω–∏|–º–æ—â–Ω|solution)', header_text.lower())):
            
            if is_definitely_company(header_text):
                normalized = normalize_company_name(header_text)
                if normalized and normalized not in companies:
                    companies.append(normalized)
    
    return companies

def extract_task_title(header_text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    title = re.sub(r'^#+\s*', '', header_text).strip()
    
    # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
    title = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', title)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    title = re.sub(r'\s+', ' ', title).strip()
    
    return title

def extract_all_from_md_file(file_path: str) -> dict:
    """–ë–µ–∑–æ—à–∏–±–æ—á–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –∑–∞–¥–∞—á–∏ –∏–∑ .md —Ñ–∞–π–ª–∞"""
    if not os.path.exists(file_path):
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return {}
    
    result = {
        'file_path': file_path,
        'tasks': [],
        'companies': [],
        'all_headers': [],
        'statistics': {}
    }
    
    if not content.strip():
        return result
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    all_headers = re.findall(r'^(#{1,6})\s*(.+)$', content, re.MULTILINE)
    for level, header_text in all_headers:
        result['all_headers'].append({
            'level': len(level),
            'text': header_text.strip(),
            'normalized': header_text.strip().lower()
        })
    
    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –±–ª–æ–∫–æ–≤ "–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤"
    company_block_patterns = [
        r'–≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤[:\s]*.*?(?=\n\S|\n#+|\n```|\Z)',
        r'–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤[:\s]*.*?(?=\n\S|\n#+|\n```|\Z)', 
        r'–≤—Å—Ç—Ä–µ—á–∞–ª—Å—è –≤[:\s]*.*?(?=\n\S|\n#+|\n```|\Z)',
        r'–ø–æ–ø–∞–¥–∞–ª–æ—Å—å –≤[:\s]*.*?(?=\n\S|\n#+|\n```|\Z)'
    ]
    
    all_companies = set()
    
    for pattern in company_block_patterns:
        blocks = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
        for block in blocks:
            companies_in_block = extract_companies_from_block(block)
            all_companies.update(companies_in_block)
    
    # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    header_companies = extract_companies_from_headers(result['all_headers'], file_path)
    all_companies.update(header_companies)
    
    result['companies'] = sorted(list(all_companies))
    
    # 4. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∑–∞–¥–∞—á–∏
    sections = re.split(r'(^#{1,6}\s+.+)$', content, flags=re.MULTILINE)
    
    current_task = None
    current_content = ""
    
    for section in sections:
        if re.match(r'^#{1,6}\s+', section):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–¥–∞—á—É
            if current_task and current_content.strip():
                task_companies = set()
                
                # –ò—â–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –∑–∞–¥–∞—á–∏
                for pattern in company_block_patterns:
                    task_blocks = re.findall(pattern, current_content, re.IGNORECASE | re.DOTALL)
                    for block in task_blocks:
                        companies_in_task = extract_companies_from_block(block)
                        task_companies.update(companies_in_task)
                
                clean_title = extract_task_title(current_task)
                if clean_title:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
                    result['tasks'].append({
                        'title': clean_title,
                        'original_title': current_task,
                        'normalized_title': clean_title.lower(),
                        'companies': sorted(list(task_companies)),
                        'content_length': len(current_content.strip()),
                        'has_code': '```' in current_content,
                        'has_companies': len(task_companies) > 0
                    })
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
            current_task = section.strip()
            current_content = ""
        else:
            current_content += section
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–¥–∞—á—É
    if current_task and current_content.strip():
        task_companies = set()
        for pattern in company_block_patterns:
            task_blocks = re.findall(pattern, current_content, re.IGNORECASE | re.DOTALL)
            for block in task_blocks:
                companies_in_task = extract_companies_from_block(block)
                task_companies.update(companies_in_task)
        
        clean_title = extract_task_title(current_task)
        if clean_title:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
            result['tasks'].append({
                'title': clean_title,
                'original_title': current_task,
                'normalized_title': clean_title.lower(),
                'companies': sorted(list(task_companies)),
                'content_length': len(current_content.strip()),
                'has_code': '```' in current_content,
                'has_companies': len(task_companies) > 0
            })
    
    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    result['statistics'] = {
        'total_headers': len(result['all_headers']),
        'total_tasks': len(result['tasks']),
        'tasks_with_companies': len([t for t in result['tasks'] if t['has_companies']]),
        'total_companies': len(result['companies']),
        'tasks_with_code': len([t for t in result['tasks'] if t['has_code']])
    }
    
    return result

def analyze_all_md_files():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ .md —Ñ–∞–π–ª—ã —Å –∏–¥–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    
    # –ü–∞–ø–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    target_dirs = [
        os.path.join('..', 'js'),
        os.path.join('..', 'react'),
        os.path.join('..', 'ts'),
    ]
    
    md_files = []
    for target_dir in target_dirs:
        if os.path.exists(target_dir):
            for root, dirs, files in os.walk(target_dir):
                for file in files:
                    if file.endswith('.md'):
                        md_files.append(os.path.join(root, file))
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ .md —Ñ–∞–π–ª–æ–≤: {len(md_files)} (js, react, ts)")
    print("=" * 80)
    
    all_results = []
    all_companies = set()
    all_tasks = []
    company_frequency = defaultdict(int)
    
    for file_path in md_files:
        print(f"\nüìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {file_path}")
        result = extract_all_from_md_file(file_path)
        
        if result and (result['tasks'] or result['companies']):
            all_results.append(result)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            for company in result['companies']:
                all_companies.add(company)
                company_frequency[company] += 1
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
            all_tasks.extend(result['tasks'])
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª—É
            stats = result['statistics']
            print(f"   üìä –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {stats['total_headers']}")
            print(f"   üìã –ó–∞–¥–∞—á: {stats['total_tasks']}")
            print(f"   üè¢ –ó–∞–¥–∞—á —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏: {stats['tasks_with_companies']}")
            print(f"   üíª –ó–∞–¥–∞—á —Å –∫–æ–¥–æ–º: {stats['tasks_with_code']}")
            print(f"   üè™ –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {stats['total_companies']}")
            
            if result['companies']:
                print(f"   üìù –ö–æ–º–ø–∞–Ω–∏–∏: {', '.join(result['companies'][:5])}")
                if len(result['companies']) > 5:
                    print(f"       ... –∏ –µ—â–µ {len(result['companies']) - 5}")
        else:
            print(f"   ‚ö™ –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª –∏–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
    
    print("\n" + "=" * 80)
    print("ÔøΩÔøΩ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_results)}")
    print(f"   üìã –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(all_tasks)}")
    print(f"   üè¢ –ó–∞–¥–∞—á —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏: {len([t for t in all_tasks if t['has_companies']])}")
    print(f"   üíª –ó–∞–¥–∞—á —Å –∫–æ–¥–æ–º: {len([t for t in all_tasks if t['has_code']])}")
    print(f"   üè™ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {len(all_companies)}")
    
    # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    print(f"\nüèÜ –¢–û–ü-20 –ö–û–ú–ü–ê–ù–ò–ô –ü–û –ß–ê–°–¢–û–¢–ï:")
    top_companies = sorted(company_frequency.items(), key=lambda x: x[1], reverse=True)[:20]
    for i, (company, count) in enumerate(top_companies, 1):
        print(f"   {i:2d}. {company} ({count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)")
    
    print(f"\nüè¢ –í–°–ï –ù–ê–ô–î–ï–ù–ù–´–ï –ö–û–ú–ü–ê–ù–ò–ò ({len(all_companies)}):")
    sorted_companies = sorted(list(all_companies))
    for i, company in enumerate(sorted_companies, 1):
        count = company_frequency[company]
        print(f"   {i:3d}. {company} ({count})")
    
    print(f"\nüìã –ü–†–ò–ú–ï–†–´ –ó–ê–î–ê–ß –° –ö–û–ú–ü–ê–ù–ò–Ø–ú–ò:")
    tasks_with_companies = [t for t in all_tasks if t['has_companies']][:20]
    for i, task in enumerate(tasks_with_companies, 1):
        companies_str = ', '.join(task['companies'][:3])
        if len(task['companies']) > 3:
            companies_str += f" (–∏ –µ—â–µ {len(task['companies']) - 3})"
        print(f"   {i:2d}. '{task['title']}' -> [{companies_str}]")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    output_data = {
        'summary': {
            'total_files': len(all_results),
            'total_tasks': len(all_tasks),
            'tasks_with_companies': len([t for t in all_tasks if t['has_companies']]),
            'tasks_with_code': len([t for t in all_tasks if t['has_code']]),
            'unique_companies': len(all_companies)
        },
        'company_frequency': dict(company_frequency),
        'top_companies': top_companies,
        'all_companies': sorted_companies,
        'all_tasks': all_tasks,
        'file_results': all_results
    }
    
    with open('extraction_results_perfect.json', 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'extraction_results_perfect.json'")
    print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï")
    
    return output_data

if __name__ == "__main__":
    analyze_all_md_files() 