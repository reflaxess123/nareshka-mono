#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ v2
–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å CSV –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ª—É—á—à—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
"""

import json
import csv
import hashlib
from pathlib import Path
from typing import List, Dict
from collections import Counter, defaultdict
import logging
import sys

# –î–ª—è Parquet
try:
    import pandas as pd
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
except ImportError:
    PARQUET_AVAILABLE = False
    print("‚ö†Ô∏è  –î–ª—è Parquet —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pandas pyarrow")

sys.stdout.reconfigure(encoding='utf-8')
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class DataMergerV2:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self):
        self.all_items = []
        self.clean_items = []
        self.stats = defaultdict(int)
        self.seen_hashes = set()
        self.company_fixes = {
            'Full': None,  # –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
            'Unknown': None  # –ë—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
        }

    def run_parsers(self):
        """–ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        logger.info("=" * 60)
        logger.info("–ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–´–• –ü–ê–†–°–ï–†–û–í")
        logger.info("=" * 60)

        # JSON –ø–∞—Ä—Å–µ—Ä (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π, –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
        from parser_json import JSONParser
        json_parser = JSONParser()
        json_path = Path(r'/sobes-data/MASSIV_GROUPED.json')

        if json_path.exists():
            json_items = json_parser.parse_file(json_path)

            json_output = Path('parsed_json.json')
            with open(json_output, 'w', encoding='utf-8') as f:
                from dataclasses import asdict
                json.dump([asdict(item) for item in json_items], f, ensure_ascii=False, indent=2)

        # Markdown –ø–∞—Ä—Å–µ—Ä V2 (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        from parser_markdown import MarkdownParserV2
        md_parser = MarkdownParserV2()
        reports_dir = Path(r'/sobes-data/reports')

        if reports_dir.exists():
            md_items = md_parser.parse_directory(reports_dir)

            md_output = Path('parsed_markdown_v2.json')
            with open(md_output, 'w', encoding='utf-8') as f:
                from dataclasses import asdict
                json.dump([asdict(item) for item in md_items], f, ensure_ascii=False, indent=2)

    def fix_company_name(self, company: str, item: Dict) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π"""
        if not company or company == 'Full':
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ original_filename –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'original_filename' in item:
                filename = item['original_filename']
                # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                if 'gazprom' in filename.lower():
                    return '–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫'
                elif 'sber' in filename.lower() or '—Å–±–µ—Ä' in filename.lower():
                    return '–°–±–µ—Ä'
                elif 'alfa' in filename.lower() or '–∞–ª—å—Ñ–∞' in filename.lower():
                    return '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫'
                elif 'yandex' in filename.lower() or '—è–Ω–¥–µ–∫—Å' in filename.lower():
                    return '–Ø–Ω–¥–µ–∫—Å'
            return 'Unknown'

        return company

    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = ' '.join(text.split())

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(text) > 3000:
            text = text[:2997] + '...'

        return text.strip()

    def is_valid_item(self, item: Dict) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø–∏—Å–∏"""
        text = item.get('text', '').strip()

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(text) < 15:
            self.stats['rejected_short'] += 1
            return False

        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä
        garbage = ['use strict', '// ---', '/*', '*/', '...', '.', '', 'undefined', 'null']
        if text in garbage:
            self.stats['rejected_garbage'] += 1
            return False

        # –ö–æ–º–ø–∞–Ω–∏—è –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å Full
        if item.get('company') == 'Full':
            item['company'] = self.fix_company_name('Full', item)

        return True

    def classify_type(self, item: Dict) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞"""
        text = item.get('text', '').lower()

        # –ö–æ–¥ - –µ—Å–ª–∏ –µ—Å—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
        code_indicators = [
            'function ', 'const ', 'let ', 'var ', 'class ',
            'return ', 'if (', 'for (', 'while (',
            '=>', '===', '!==', '```'
        ]
        if any(indicator in text for indicator in code_indicators):
            return 'code'

        # –ó–∞–¥–∞—á–∏ - –∏–º–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
        task_keywords = [
            '–Ω–∞–ø–∏—à–∏', '–Ω–∞–ø–∏—à–∏—Ç–µ', '—Ä–µ–∞–ª–∏–∑—É–π', '—Ä–µ–∞–ª–∏–∑—É–π—Ç–µ',
            '—Å–æ–∑–¥–∞–π', '—Å–æ–∑–¥–∞–π—Ç–µ', 'implement', 'write',
            '—Ä–µ—à–∏', '—Ä–µ—à–∏—Ç–µ', '–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π', '–∏—Å–ø—Ä–∞–≤—å',
            '–¥–æ—Ä–∞–±–æ—Ç–∞–π', '–¥–æ–±–∞–≤—å', '–∏–∑–º–µ–Ω–∏'
        ]
        if any(keyword in text for keyword in task_keywords):
            return 'task'

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –≤–æ–ø—Ä–æ—Å
        return 'question'

    def merge_data(self):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("\n" + "=" * 60)
        logger.info("–û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•")
        logger.info("=" * 60)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        json_data = []
        json_path = Path('parsed_json.json')
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

        md_data = []
        md_path = Path('parsed_markdown_v2.json')
        if md_path.exists():
            with open(md_path, 'r', encoding='utf-8') as f:
                md_data = json.load(f)

        self.all_items = json_data + md_data
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.all_items)} –∑–∞–ø–∏—Å–µ–π")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        for item in self.all_items:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            item['text'] = self.clean_text(item['text'])

            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–π
            item['company'] = self.fix_company_name(item.get('company', ''), item)

            # –ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if item.get('type') == 'question' and len(item['text']) > 50:
                item['type'] = self.classify_type(item)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        valid_items = [item for item in self.all_items if self.is_valid_item(item)]
        logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(valid_items)} –∑–∞–ø–∏—Å–µ–π")

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        seen = set()
        unique_items = []
        for item in valid_items:
            # –°–æ–∑–¥–∞–µ–º —Ö–µ—à
            text_normalized = item['text'].lower().strip()
            text_hash = hashlib.md5(text_normalized.encode('utf-8')).hexdigest()

            if text_hash not in seen:
                seen.add(text_hash)
                unique_items.append(item)
                self.stats[f"type_{item['type']}"] += 1
                self.stats[f"source_{item['source']}"] += 1
            else:
                self.stats['duplicates_removed'] += 1

        self.clean_items = unique_items
        logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(self.clean_items)} –∑–∞–ø–∏—Å–µ–π")

    def export_csv(self, output_path: Path):
        """–ö–†–ò–¢–ò–ß–ï–°–ö–ò –£–õ–£–ß–®–ï–ù–ù–´–ô —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV"""
        logger.info(f"\n–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV: {output_path}")

        fieldnames = [
            'interview_id', 'company', 'source', 'type', 'text',
            'date', 'time', 'salary_range', 'level', 'duration',
            'topics', 'complexity', 'question_number', 'sender'
        ]

        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(
                f,
                fieldnames=fieldnames,
                quoting=csv.QUOTE_ALL,  # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ –ø–æ–ª—è
                quotechar='"',
                escapechar='\\',
                doublequote=True  # –£–¥–≤–∞–∏–≤–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤–Ω—É—Ç—Ä–∏ –ø–æ–ª–µ–π
            )
            writer.writeheader()

            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–ª—è –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é
            rows_to_write = []
            for item in self.clean_items:
                row = {field: item.get(field, '') for field in fieldnames}
                rows_to_write.append(row)

            writer.writerows(rows_to_write)

        logger.info(f"‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(self.clean_items)} –∑–∞–ø–∏—Å–µ–π")

    def export_parquet(self, output_path: Path):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet"""
        if not PARQUET_AVAILABLE:
            logger.warning("‚ö†Ô∏è  Parquet –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        logger.info(f"\n–≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet: {output_path}")

        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(self.clean_items)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        columns_to_keep = [
            'interview_id', 'company', 'source', 'type', 'text',
            'date', 'time', 'salary_range', 'level', 'duration',
            'topics', 'complexity', 'question_number', 'sender'
        ]

        for col in columns_to_keep:
            if col not in df.columns:
                df[col] = ''

        df = df[columns_to_keep]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        df.to_parquet(output_path, engine='pyarrow', compression='snappy', index=False)

        size_mb = output_path.stat().st_size / 1024 / 1024
        logger.info(f"‚úÖ Parquet —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π ({size_mb:.2f} MB)")

    def print_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        logger.info("\n" + "=" * 60)
        logger.info("–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("=" * 60)

        logger.info(f"\nüìä –ò—Ç–æ–≥–∏:")
        logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.clean_items)}")
        logger.info(f"  –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {self.stats['duplicates_removed']}")
        logger.info(f"  –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö: {self.stats['rejected_short']}")

        logger.info(f"\nüìù –ü–æ —Ç–∏–ø–∞–º:")
        logger.info(f"  –í–æ–ø—Ä–æ—Å–æ–≤: {self.stats['type_question']}")
        logger.info(f"  –ó–∞–¥–∞—á: {self.stats['type_task']}")
        logger.info(f"  –ö–æ–¥–∞: {self.stats['type_code']}")

        logger.info(f"\nüìÅ –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        logger.info(f"  –ò–∑ JSON: {self.stats['source_json']}")
        logger.info(f"  –ò–∑ Markdown: {self.stats['source_markdown']}")

        # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π
        companies = Counter(item['company'] for item in self.clean_items)
        logger.info(f"\nüè¢ –¢–æ–ø-10 –∫–æ–º–ø–∞–Ω–∏–π:")
        for company, count in companies.most_common(10):
            pct = count * 100 / len(self.clean_items)
            logger.info(f"  {company}: {count} ({pct:.1f}%)")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º
        full_count = companies.get('Full', 0)
        unknown_count = companies.get('Unknown', 0)

        if full_count > 0:
            logger.warning(f"\n‚ö†Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å {full_count} –∑–∞–ø–∏—Å–µ–π —Å 'Full'")
        if unknown_count > 100:
            logger.warning(f"‚ö†Ô∏è  {unknown_count} –∑–∞–ø–∏—Å–µ–π —Å 'Unknown'")
        else:
            logger.info(f"\n‚úÖ –ü—Ä–æ–±–ª–µ–º–∞ —Å 'Full' –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞!")
            logger.info(f"‚úÖ Unknown —Å–æ–∫—Ä–∞—â–µ–Ω–æ –¥–æ {unknown_count} –∑–∞–ø–∏—Å–µ–π")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("=" * 60)
    logger.info("–£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• v2")
    logger.info("=" * 60)

    merger = DataMergerV2()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä—ã
    merger.run_parsers()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    merger.merge_data()

    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º
    csv_path = Path('interview_questions_v2.csv')
    merger.export_csv(csv_path)

    parquet_path = Path('interview_questions_v2.parquet')
    merger.export_parquet(parquet_path)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    merger.print_statistics()

    logger.info("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    main()
