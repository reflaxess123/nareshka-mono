#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–õ–£–ß–®–ï–ù–ù–ê–Ø GPT –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê V2
- GPT-4.1-mini –≤–º–µ—Å—Ç–æ 4o-mini
- –õ—É—á—à–∏–µ –ø—Ä–æ–º–ø—Ç—ã
- –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import pandas as pd
import os
import time
import requests
from typing import List, Dict

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API
API_KEY = "sk-yseNQGJXYUnn4YjrnwNJnwW7bsnwFg8K"
API_BASE_URL = "https://api.proxyapi.ru/openai/v1"

def call_gpt(prompt: str, model: str = "gpt-4.1-mini") -> str:
    """–í—ã–∑–æ–≤ GPT —á–µ—Ä–µ–∑ ProxyAPI"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ
        "max_tokens": 50      # –ö–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    }
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ API: {e}")
        return "–¢–µ–º–∞"

def improve_topic_name(old_name: str, keywords: str, sample_questions: List[str]) -> str:
    """–£–ª—É—á—à–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º—ã"""
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä–≤—å—é. –£–ª—É—á—à–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã –¥–ª—è IT —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–π.

–¢–ï–ö–£–©–ï–ï –ù–ê–ó–í–ê–ù–ò–ï: "{old_name}"
–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {keywords}
–ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í:
‚Ä¢ {sample_questions[0] if sample_questions else '–ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤'}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚úì 1-3 —Å–ª–æ–≤–∞ –º–∞–∫—Å–∏–º—É–º
‚úì –¢–µ—Ö–Ω–∏—á–Ω–æ–µ –∏ —Ç–æ—á–Ω–æ–µ (Event Loop, React Hooks, CSS Grid)
‚úì –ù–∞ —Ä—É—Å—Å–∫–æ–º, –Ω–æ –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –∞–Ω–≥–ª–∏—Ü–∏–∑–º—ã –µ—Å–ª–∏ –æ–Ω–∏ —É—Å—Ç–æ—è–≤—à–∏–µ—Å—è
‚úì –£–±–µ—Ä–∏ –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞ "–∞—Å–ø–µ–∫—Ç—ã", "–≤–æ–ø—Ä–æ—Å—ã", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"

–£–õ–£–ß–®–ï–ù–ù–û–ï –ù–ê–ó–í–ê–ù–ò–ï:"""
    
    improved = call_gpt(prompt).strip().strip('"').strip("'")
    return improved if improved != "–¢–µ–º–∞" else old_name

def fix_wrong_categories():
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞"""
    fixes = {
        "–ü–æ–∏—Å–∫ –Ω–æ–≤–æ–≥–æ –º–µ—Å—Ç–∞ —Ä–∞–±–æ—Ç—ã": "Soft Skills",
        "–ß–∏—Å–ª–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –º–∞—Å—Å–∏–≤–∞": "–ê–ª–≥–æ—Ä–∏—Ç–º—ã", 
        "–¶–∏–∫–ª—ã —Å–æ–±—ã—Ç–∏–π –≤ –º–∏–∫—Ä–æ–æ–±—Ä–∞–±–æ—Ç–∫–µ": "JavaScript Core",
        "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–µ–±-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "JavaScript Core",
        "–ö–æ–¥-—Ä–µ–≤—å—é –∏ –≤–Ω–∏–º–∞–Ω–∏–µ": "Soft Skills",
        "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": "JavaScript Core",
        "–ë—Ä–∞—É–∑–µ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞": "JavaScript Core",
    }
    return fixes

def main():
    print("üîß –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê V2")
    print("=" * 50)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    input_file = "outputs_bertopic_enhanced/clusters_enhanced.csv"
    if not os.path.exists(input_file):
        print("‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ –±–∞–∑–æ–≤—É—é –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É!")
        return
    
    df = pd.read_csv(input_file)
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    # 2. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    print("\nüè∑Ô∏è  –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
    category_fixes = fix_wrong_categories()
    
    for idx, row in df.iterrows():
        name = row['human_name']
        keywords = row['keywords']
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        if name in category_fixes:
            df.at[idx, 'category'] = category_fixes[name]
            print(f"   ‚úì {name} ‚Üí {category_fixes[name]}")
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞–≤—Ç–æ–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        elif row['category'] == '–î—Ä—É–≥–æ–µ':
            new_category = smart_categorize(name, keywords)
            if new_category != '–î—Ä—É–≥–æ–µ':
                df.at[idx, 'category'] = new_category
                print(f"   ‚úì {name} ‚Üí {new_category}")
    
    # 3. –£–ª—É—á—à–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ç–µ–º
    print("\nüìù –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π...")
    problem_names = [
        "–•—É–∫–∏ –∏ —Ñ–∞–∫–∞–ø—ã", 
        "–¶–∏–∫–ª—ã —Å–æ–±—ã—Ç–∏–π –≤ –º–∏–∫—Ä–æ–æ–±—Ä–∞–±–æ—Ç–∫–µ",
        "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–µ–±-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "–ü–æ–∏—Å–∫ –Ω–æ–≤–æ–≥–æ –º–µ—Å—Ç–∞ —Ä–∞–±–æ—Ç—ã"
    ]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –æ–¥–∏–Ω —Ä–∞–∑
    questions_df = pd.read_csv("outputs_bertopic/questions_with_clusters.csv")
    
    for idx, row in df.iterrows():
        if row['human_name'] in problem_names:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_questions = questions_df[
                questions_df['cluster_id'] == row['cluster_id']
            ]['question_text'].head(3).tolist()
            
            improved_name = improve_topic_name(
                row['human_name'], 
                row['keywords'], 
                cluster_questions
            )
            
            if improved_name != row['human_name']:
                df.at[idx, 'human_name'] = improved_name
                print(f"   ‚úì {row['human_name']} ‚Üí {improved_name}")
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É API –≤—ã–∑–æ–≤–∞–º–∏
    
    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = "outputs_bertopic_v2"
    os.makedirs(output_dir, exist_ok=True)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
    df.to_csv(f"{output_dir}/clusters_enhanced_v2.csv", index=False, encoding='utf-8-sig')
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_dir}/clusters_enhanced_v2.csv")
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_summary = df.groupby('category').agg({
        'size': 'sum',
        'cluster_id': 'count'
    }).rename(columns={'cluster_id': 'clusters_count'}).sort_values('size', ascending=False)
    
    category_summary.to_csv(f"{output_dir}/category_summary_v2.csv", encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_dir}/category_summary_v2.csv")
    
    # –¢–æ–ø —Ç–µ–º—ã
    top_topics = df.nlargest(20, 'size')[['human_name', 'category', 'size', 'keywords']]
    top_topics.to_csv(f"{output_dir}/top_topics_v2.csv", index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_dir}/top_topics_v2.csv")
    
    print("\n" + "‚úÖ" * 30)
    print("–£–õ–£–ß–®–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {output_dir}/")
    print("‚úÖ" * 30)

def smart_categorize(name: str, keywords: str) -> str:
    """–£–º–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    name_lower = name.lower()
    keywords_lower = keywords.lower()
    
    patterns = {
        'JavaScript Core': ['event', 'loop', '–ø—Ä–æ–º–∏—Å', '–∞—Å–∏–Ω—Ö—Ä–æ–Ω', '–∑–∞–º—ã–∫–∞–Ω', 'this', 'prototype'],
        'React': ['—Ö—É–∫', 'hook', 'react', '–∫–æ–º–ø–æ–Ω–µ–Ω—Ç', 'jsx', 'props', 'state', 'redux'],
        'TypeScript': ['typescript', '—Ç–∏–ø', 'type', 'interface', 'generic'],
        '–í–µ—Ä—Å—Ç–∫–∞': ['css', 'html', 'flex', 'grid', 'layout', '–∞–Ω–∏–º–∞—Ü'],
        '–ê–ª–≥–æ—Ä–∏—Ç–º—ã': ['–∞–ª–≥–æ—Ä–∏—Ç–º', '—Å–ª–æ–∂–Ω–æ—Å—Ç—å', '–º–∞—Å—Å–∏–≤', '—á–∏—Å–ª', '—Å–æ—Ä—Ç–∏—Ä', '–ø–æ–∏—Å–∫'],
        'Soft Skills': ['–æ–ø—ã—Ç', '–∫–æ–º–∞–Ω–¥', '–ø—Ä–æ–µ–∫—Ç', '—Ä–∞–±–æ—Ç', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–∫–∞—Ä—å–µ—Ä'],
        '–°–µ—Ç—å': ['http', 'api', 'cors', 'rest', 'websocket', '–∑–∞–ø—Ä–æ—Å'],
        '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ': ['—Ç–µ—Å—Ç', 'unit', 'jest', 'mock'],
    }
    
    for category, words in patterns.items():
        if any(word in name_lower or word in keywords_lower for word in words):
            return category
    
    return '–î—Ä—É–≥–æ–µ'

if __name__ == "__main__":
    main()