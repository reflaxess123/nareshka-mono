#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BERTopic Interview Analysis - –ó–ê–ú–ï–ù–ê –ú–£–°–û–†–ù–û–ì–û run_analysis.py
–ê–Ω–∞–ª–∏–∑ 8.5k –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–Ω—Ç–µ—Ä–≤—å—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
"""
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import argparse
import os
import json
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import numpy as np
from tqdm import tqdm

# BERTopic –∏ ML
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
import plotly.graph_objects as go
import plotly.express as px

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt


def normalize_company(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π"""
    if not isinstance(name, str):
        return ""
    import re
    norm = name.strip()
    norm = re.sub(r"^[-‚Äì‚Äî\s]+", "", norm)
    norm = re.sub(r"\s+", " ", norm)
    return norm


def load_interview_data(input_csv: str) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        df = pd.read_csv(input_csv, encoding='utf-8-sig')
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {input_csv}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_cols = {'id', 'question_text', 'company', 'date', 'interview_id'}
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        df['company_norm'] = df['company'].apply(normalize_company)
        df['question_text_norm'] = df['question_text'].astype(str).str.strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        df = df[df['question_text_norm'].str.len() > 5].reset_index(drop=True)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df)} –≤–∞–ª–∏–¥–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
        
        return df
        
    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")


def create_bertopic_model(
    language: str = "multilingual",
    min_topic_size: int = 10,
    nr_topics: str = "auto",
    embedding_model_name: str = "paraphrase-multilingual-mpnet-base-v2",
    verbose: bool = True
) -> BERTopic:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERTopic –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    print("ü§ñ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ BERTopic –º–æ–¥–µ–ª–∏...")
    print(f"   üìù –Ø–∑—ã–∫: {language}")
    print(f"   üéØ –ú–∏–Ω–∏–º—É–º –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ —Ç–µ–º–µ: {min_topic_size}")
    print(f"   üß† –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_model_name}")
    
    # –ö–∞—Å—Ç–æ–º–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    embedding_model = SentenceTransformer(embedding_model_name)
    
    # UMAP –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    umap_model = UMAP(
        n_neighbors=15, 
        n_components=5, 
        min_dist=0.0, 
        metric='cosine',
        random_state=42
    )
    
    # HDBSCAN –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    hdbscan_model = HDBSCAN(
        min_cluster_size=min_topic_size,
        min_samples=min(5, min_topic_size // 2),  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π min_samples
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True  # –í–∞–∂–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    topic_model = BERTopic(
        embedding_model=embedding_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        language=language,
        calculate_probabilities=True,
        verbose=verbose
    )
    
    print("‚úÖ –ú–æ–¥–µ–ª—å BERTopic –≥–æ—Ç–æ–≤–∞!")
    return topic_model


def analyze_topics(
    df: pd.DataFrame, 
    topic_model: BERTopic,
    output_dir: str
) -> tuple:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º"""
    print("\nüîç –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –¢–ï–ú–ê–¢–ò–ö...")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    texts = df['question_text_norm'].tolist()
    
    print(f"üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(texts)} –≤–æ–ø—Ä–æ—Å–æ–≤...")
    
    # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è
    start_time = time.time()
    
    # üöÄ –ì–õ–ê–í–ù–ê–Ø –ú–ê–ì–ò–Ø - –í–°–ï–ì–û –û–î–ù–ê –°–¢–†–û–ß–ö–ê –í–ú–ï–°–¢–û 812!
    topics, probabilities = topic_model.fit_transform(texts)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.1f}—Å!")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    unique_topics = len(set(topics))
    outliers = list(topics).count(-1)
    coverage = (len(topics) - outliers) / len(topics) * 100
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   üéØ –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {unique_topics}")
    print(f"   üìà –ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage:.1f}% ({len(topics) - outliers}/{len(topics)})")
    print(f"   üóëÔ∏è  –í—ã–±—Ä–æ—Å—ã: {outliers}")
    
    return topics, probabilities


def export_results(
    df: pd.DataFrame, 
    topics: List[int],
    probabilities: np.ndarray,
    topic_model: BERTopic,
    output_dir: str
):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ —á—Ç–æ –∏ —Å—Ç–∞—Ä—ã–π —Å–∫—Ä–∏–ø—Ç"""
    print("\nüíæ –≠–ö–°–ü–û–†–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    df_results = df.copy()
    df_results['cluster_id'] = topics
    df_results['topic_probability'] = probabilities.max(axis=1) if probabilities is not None else 0.0
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –≤—ã–±—Ä–æ—Å—ã (-1) –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    valid_clusters = [t for t in topics if t != -1]
    coverage = len(valid_clusters) / len(topics) * 100 if topics else 0
    
    # 2. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–º–∞—Ö
    topic_info = topic_model.get_topic_info()
    
    # 3. –°–æ–∑–¥–∞–µ–º cluster_labels –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ
    cluster_labels = []
    for _, row in topic_info.iterrows():
        topic_id = int(row['Topic'])
        if topic_id == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
            continue
            
        # –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å - —Å–∞–º—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—ã–π –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
        topic_docs = [df_results.iloc[i]['question_text_norm'] 
                     for i, t in enumerate(topics) if t == topic_id]
        canonical_question = topic_docs[0] if topic_docs else "No representative question"
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–∞–∫ —Ç–µ–º—ã
        keywords = ', '.join([word for word, _ in topic_model.get_topic(topic_id)[:5]])
        
        cluster_labels.append({
            'cluster_id': topic_id,
            'canonical_question': canonical_question,
            'topics': keywords,
            'size': row['Count'],
            'topic_confidence': f'{{"main_topic": 1.0}}',
            'fingerprint': hashlib.md5(canonical_question.encode()).hexdigest()[:16]
        })
    
    # 4. –¢–æ–ø —Ç–µ–º—ã –≥–ª–æ–±–∞–ª—å–Ω–æ
    topic_counts = pd.Series([t for t in topics if t != -1]).value_counts()
    top_topics = []
    for topic_id, count in topic_counts.head(20).items():
        keywords = ', '.join([word for word, _ in topic_model.get_topic(topic_id)[:3]])
        top_topics.append({
            'topic': keywords,
            'interviews_count': count
        })
    
    # 5. –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
    company_analysis = []
    for company in df_results['company_norm'].unique():
        if pd.isna(company) or company == '':
            continue
        company_data = df_results[df_results['company_norm'] == company]
        company_topics = company_data['cluster_id'].value_counts().head(10)
        for topic_id, count in company_topics.items():
            if topic_id != -1:
                company_analysis.append({
                    'company_norm': company,
                    'cluster_id': topic_id,
                    'interviews_count': count
                })
    
    # –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ê–ô–õ–û–í
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    output_file = os.path.join(output_dir, "questions_with_clusters.csv")
    df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"   ‚úÖ {output_file}")
    
    # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã 
    enriched_file = os.path.join(output_dir, "enriched_questions.csv")
    df_enriched = df_results.merge(
        pd.DataFrame(cluster_labels)[['cluster_id', 'canonical_question', 'topics']],
        on='cluster_id', 
        how='left'
    )
    df_enriched.to_csv(enriched_file, index=False, encoding='utf-8-sig')
    print(f"   ‚úÖ {enriched_file}")
    
    # –û–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    labels_file = os.path.join(output_dir, "cluster_labels.csv")
    pd.DataFrame(cluster_labels).to_csv(labels_file, index=False, encoding='utf-8-sig')
    print(f"   ‚úÖ {labels_file}")
    
    # –¢–æ–ø —Ç–µ–º—ã
    topics_file = os.path.join(output_dir, "top_topics_global.csv")
    pd.DataFrame(top_topics).to_csv(topics_file, index=False, encoding='utf-8-sig')
    print(f"   ‚úÖ {topics_file}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
    companies_file = os.path.join(output_dir, "by_company_top_clusters.csv")
    pd.DataFrame(company_analysis).to_csv(companies_file, index=False, encoding='utf-8-sig')
    print(f"   ‚úÖ {companies_file}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_file = os.path.join(output_dir, "bertopic_model")
    topic_model.save(model_file)
    print(f"   ‚úÖ {model_file}")
    
    print(f"\nüéâ –í–°–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´ –í: {output_dir}")
    
    return {
        'total_questions': len(df_results),
        'valid_topics': len([t for t in topics if t != -1]),
        'coverage_percent': coverage,
        'num_topics': len(set(topics)) - (1 if -1 in topics else 0)
    }


def create_visualizations(
    topic_model: BERTopic,
    output_dir: str
):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    print("\nüìä –°–û–ó–î–ê–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô...")
    
    try:
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ–º
        fig1 = topic_model.visualize_heatmap(width=1000, height=600)
        fig1.write_html(os.path.join(output_dir, "topics_heatmap.html"))
        print(f"   ‚úÖ topics_heatmap.html")
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ–º
        fig2 = topic_model.visualize_topics(width=1200, height=800)
        fig2.write_html(os.path.join(output_dir, "topics_visualization.html"))
        print(f"   ‚úÖ topics_visualization.html")
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ç–µ–º
        fig3 = topic_model.visualize_barchart(top_k_topics=20, width=800, height=600)
        fig3.write_html(os.path.join(output_dir, "topics_barchart.html"))
        print(f"   ‚úÖ topics_barchart.html")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="üöÄ BERTopic Interview Analysis - –ù–û–í–û–ï –ü–û–ö–û–õ–ï–ù–ò–ï"
    )
    parser.add_argument("--input", required=True, help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏")
    parser.add_argument("--output", default="outputs_bertopic", help="–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--min-topic-size", type=int, default=10, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ–º—ã")
    parser.add_argument("--language", default="multilingual", help="–Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--embedding-model", default="paraphrase-multilingual-mpnet-base-v2", help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    parser.add_argument("--visualizations", action="store_true", help="–°–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
    
    args = parser.parse_args()
    
    print("üöÄ" * 20)
    print("üöÄ BERTopic INTERVIEW ANALYSIS")
    print("üöÄ –ó–ê–ú–ï–ù–ê –ú–£–°–û–†–ù–û–ì–û run_analysis.py")
    print("üöÄ" * 20)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = load_interview_data(args.input)
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        topic_model = create_bertopic_model(
            language=args.language,
            min_topic_size=args.min_topic_size,
            embedding_model_name=args.embedding_model
        )
        
        # 3. –ê–Ω–∞–ª–∏–∑
        topics, probabilities = analyze_topics(df, topic_model, args.output)
        
        # 4. –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        stats = export_results(df, topics, probabilities, topic_model, args.output)
        
        # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if args.visualizations:
            create_visualizations(topic_model, args.output)
        
        print("\n" + "üéâ" * 50)
        print("üéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_questions']} –≤–æ–ø—Ä–æ—Å–æ–≤")
        print(f"üéâ –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {stats['num_topics']}")
        print(f"üéâ –ü–æ–∫—Ä—ã—Ç–∏–µ: {stats['coverage_percent']:.1f}%")
        print(f"üéâ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {args.output}")
        print("üéâ" * 50)
        
    except Exception as e:
        print(f"\nüí• –û–®–ò–ë–ö–ê: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())