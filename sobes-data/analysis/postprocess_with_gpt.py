#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ BERTopic —Å –ø–æ–º–æ—â—å—é GPT
–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞
"""

import pandas as pd
import json
import os
from typing import List, Dict
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API
API_KEY = "sk-yseNQGJXYUnn4YjrnwNJnwW7bsnwFg8K"  # –¢–≤–æ–π ProxyAPI –∫–ª—é—á
API_BASE_URL = "https://api.proxyapi.ru/openai/v1"  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π URL ProxyAPI

def call_gpt(prompt: str, model: str = "gpt-4.1-mini") -> str:
    """–í—ã–∑–æ–≤ GPT —á–µ—Ä–µ–∑ ProxyAPI"""
    import requests
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "max_tokens": 100
    }
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ API: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return prompt.split(',')[0].strip() if ',' in prompt else "–¢–µ–º–∞"

def generate_topic_name(keywords: str, sample_questions: List[str], size: int) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º—ã"""
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä–≤—å—é –≤ IT. –î–∞–π —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Ç–µ–º—ã –∏–∑ {size} –≤–æ–ø—Ä–æ—Å–æ–≤.

–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {keywords}

–ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í:
{chr(10).join(f'‚Ä¢ {q}' for q in sample_questions[:3])}

–ü–†–ê–í–ò–õ–ê:
‚úì 2-4 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
‚úì –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "Event Loop", "React —Ö—É–∫–∏", "TypeScript —Ç–∏–ø—ã")
‚úì –ë–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "–∏ –µ–≥–æ –∞—Å–ø–µ–∫—Ç—ã", "–≤–æ–ø—Ä–æ—Å—ã", "–∏–Ω—Ç–µ—Ä–≤—å—é"
‚úì –ë–ï–ó –∫–∞–≤—ã—á–µ–∫

–¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã:"""
    
    return call_gpt(prompt).strip().strip('"')

def find_duplicate_clusters(clusters_df: pd.DataFrame) -> List[List[int]]:
    """–ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–ª–∞—Å—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ GPT"""
    # –ë–µ—Ä–µ–º —Ç–æ–ø-50 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    top_clusters = clusters_df.head(50)
    
    clusters_info = []
    for _, row in top_clusters.iterrows():
        clusters_info.append({
            "id": row['cluster_id'],
            "keywords": row['topics'],
            "size": row['size']
        })
    
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ —Ç–µ–º—ã –∏–Ω—Ç–µ—Ä–≤—å—é –∏ –Ω–∞–π–¥–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è/–ø–æ—Ö–æ–∂–∏–µ.

{json.dumps(clusters_info, ensure_ascii=False, indent=2)}

–ù–∞–π–¥–∏ –≥—Ä—É–ø–ø—ã —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã TypeScript –∏–ª–∏ event loop).
–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ –≥—Ä—É–ø–ø: [[id1, id2], [id3, id4, id5], ...]
–¢–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –¥—É–±–ª–∏!

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."""
    
    response = call_gpt(prompt)
    try:
        return json.loads(response)
    except:
        return []

def postprocess_bertopic_results(
    input_dir: str = "outputs_bertopic",
    output_dir: str = "outputs_bertopic_enhanced"
):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    print("="*60)
    print("üöÄ –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í BERTOPIC")
    print("="*60)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤...")
    clusters_df = pd.read_csv(f"{input_dir}/cluster_labels.csv")
    questions_df = pd.read_csv(f"{input_dir}/questions_with_clusters.csv")
    
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(clusters_df)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions_df)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–µ–º —á–µ—Ä–µ–∑ GPT...")
    print("   (—ç—Ç–æ –∑–∞–π–º–µ—Ç 2-3 –º–∏–Ω—É—Ç—ã)")
    
    enhanced_clusters = []
    
    for idx, row in clusters_df.iterrows():
        cluster_id = row['cluster_id']
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        cluster_questions = questions_df[
            questions_df['cluster_id'] == cluster_id
        ]['question_text'].head(5).tolist()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        try:
            human_name = generate_topic_name(
                row['topics'], 
                cluster_questions,
                row['size']
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = classify_topic_category(human_name, row['topics'])
            
            enhanced_clusters.append({
                'cluster_id': cluster_id,
                'human_name': human_name,
                'category': category,
                'keywords': row['topics'],
                'size': row['size'],
                'canonical_question': row['canonical_question']
            })
            
            if idx % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx}/{len(clusters_df)} —Ç–µ–º...")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
            enhanced_clusters.append({
                'cluster_id': cluster_id,
                'human_name': row['topics'].split(',')[0].strip(),
                'category': '–î—Ä—É–≥–æ–µ',
                'keywords': row['topics'],
                'size': row['size'],
                'canonical_question': row['canonical_question']
            })
        
        # –ü–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
        if idx % 5 == 0:
            time.sleep(1)
    
    # 3. –ù–∞—Ö–æ–¥–∏–º –∏ –ø–æ–º–µ—á–∞–µ–º –¥—É–±–ª–∏
    print("\nüîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ–º...")
    duplicates = find_duplicate_clusters(clusters_df)
    
    if duplicates:
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–µ–π")
        # –ü–æ–º–µ—á–∞–µ–º –¥—É–±–ª–∏
        for group in duplicates:
            main_id = group[0]
            for dup_id in group[1:]:
                for cluster in enhanced_clusters:
                    if cluster['cluster_id'] == dup_id:
                        cluster['duplicate_of'] = main_id
    
    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    enhanced_df = pd.DataFrame(enhanced_clusters)
    enhanced_df.to_csv(f"{output_dir}/clusters_enhanced.csv", index=False, encoding='utf-8-sig')
    print(f"   ‚úì {output_dir}/clusters_enhanced.csv")
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_summary = enhanced_df.groupby('category').agg({
        'size': 'sum',
        'cluster_id': 'count'
    }).rename(columns={'cluster_id': 'clusters_count'}).sort_values('size', ascending=False)
    
    category_summary.to_csv(f"{output_dir}/category_summary.csv", encoding='utf-8-sig')
    print(f"   ‚úì {output_dir}/category_summary.csv")
    
    # –¢–æ–ø —Ç–µ–º—ã —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    top_topics = enhanced_df.nlargest(20, 'size')[['human_name', 'category', 'size', 'keywords']]
    top_topics.to_csv(f"{output_dir}/top_topics_human.csv", index=False, encoding='utf-8-sig')
    print(f"   ‚úì {output_dir}/top_topics_human.csv")
    
    print("\n" + "="*60)
    print("‚úÖ –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {output_dir}/")
    print("="*60)
    
    return enhanced_df

def classify_topic_category(name: str, keywords: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
    name_lower = name.lower()
    keywords_lower = keywords.lower()
    
    # –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    exact_matches = {
        'event loop': 'JavaScript Core',
        'event-loop': 'JavaScript Core', 
        '–ø—Ä–æ–º–∏—Å': 'JavaScript Core',
        '–∞—Å–∏–Ω—Ö—Ä–æ–Ω': 'JavaScript Core',
        '–∑–∞–º—ã–∫–∞–Ω': 'JavaScript Core',
        '–ø—Ä–æ—Ç–æ—Ç–∏–ø': 'JavaScript Core',
        '—Ö—É–∫': 'React',
        'redux': 'React',
        'react': 'React',
        '—Å–æ—Å—Ç–æ—è–Ω': 'React',
        '–∫–æ–º–ø–æ–Ω–µ–Ω—Ç': 'React',
        '—Ç–∏–ø—ã': 'TypeScript',
        'typescript': 'TypeScript',
        'interface': 'TypeScript',
        '–¥–∂–µ–Ω–µ—Ä': 'TypeScript',
        'css': '–í–µ—Ä—Å—Ç–∫–∞',
        'html': '–í–µ—Ä—Å—Ç–∫–∞',
        'flex': '–í–µ—Ä—Å—Ç–∫–∞',
        'grid': '–í–µ—Ä—Å—Ç–∫–∞',
        '–∞–ª–≥–æ—Ä–∏—Ç–º': '–ê–ª–≥–æ—Ä–∏—Ç–º—ã',
        '—Å–ª–æ–∂–Ω–æ—Å—Ç—å': '–ê–ª–≥–æ—Ä–∏—Ç–º—ã',
        '–º–∞—Å—Å–∏–≤': '–ê–ª–≥–æ—Ä–∏—Ç–º—ã',
        '—á–∏—Å–ª': '–ê–ª–≥–æ—Ä–∏—Ç–º—ã',
        'http': '–°–µ—Ç—å',
        'cors': '–°–µ—Ç—å',
        'api': '–°–µ—Ç—å',
        'websocket': '–°–µ—Ç—å',
        '—Ç–µ—Å—Ç': '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ',
        'unit': '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ',
        '–∫–æ–º–∞–Ω–¥': 'Soft Skills',
        '–æ–ø—ã—Ç': 'Soft Skills',
        '–ø—Ä–æ–µ–∫—Ç': 'Soft Skills',
        '—Ä–∞–±–æ—Ç': 'Soft Skills',
        '—Ä–∞–∑–≤–∏—Ç–∏–µ': 'Soft Skills',
        'git': '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã',
        'webpack': '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã',
        'npm': '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã',
        'node': 'Node.js',
        'express': 'Node.js',
        'backend': 'Node.js',
        '–ø–∞—Ç—Ç–µ—Ä–Ω': '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞',
        'solid': '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞',
        '–∞—Ä—Ö–∏—Ç–µ–∫—Ç': '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞',
    }
    
    # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    for pattern, category in exact_matches.items():
        if pattern in name_lower or pattern in keywords_lower:
            return category
    
    return '–î—Ä—É–≥–æ–µ'

if __name__ == "__main__":
    postprocess_bertopic_results()