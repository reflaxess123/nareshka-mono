import pandas as pd
import numpy as np
from sklearn.cluster import HDBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter

def test_hdbscan_params(csv_path):
    """–ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã HDBSCAN"""
    df = pd.read_csv(csv_path)
    texts = df['question_text'].astype(str).fillna('').tolist()
    
    # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞
    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')
    vectors = vectorizer.fit_transform(texts).toarray()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    params_to_test = [
        {"min_cluster_size": 3, "min_samples": 2},   # –õ–∏–±–µ—Ä–∞–ª—å–Ω—ã–µ
        {"min_cluster_size": 5, "min_samples": 3},   # –£–º–µ—Ä–µ–Ω–Ω—ã–µ  
        {"min_cluster_size": 8, "min_samples": 4},   # –°—Ç—Ä–æ–≥–∏–µ
        {"min_cluster_size": 12, "min_samples": 6},  # –û—á–µ–Ω—å —Å—Ç—Ä–æ–≥–∏–µ
    ]
    
    results = []
    
    for params in params_to_test:
        clusterer = HDBSCAN(
            min_cluster_size=params["min_cluster_size"],
            min_samples=params["min_samples"],
            metric='euclidean',
            cluster_selection_method='eom'
        )
        
        cluster_labels = clusterer.fit_predict(vectors)
        
        # –ê–Ω–∞–ª–∏–∑
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        coverage = (len(texts) - n_noise) / len(texts) * 100
        
        cluster_sizes = Counter([label for label in cluster_labels if label != -1])
        max_size = max(cluster_sizes.values()) if cluster_sizes else 0
        avg_size = np.mean(list(cluster_sizes.values())) if cluster_sizes else 0
        
        result = {
            **params,
            "n_clusters": n_clusters,
            "coverage_percent": round(coverage, 1),
            "max_cluster_size": max_size,
            "avg_cluster_size": round(avg_size, 1),
            "noise_points": n_noise
        }
        results.append(result)
        
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage:.1f}%")
        print(f"  –ú–∞–∫—Å —Ä–∞–∑–º–µ—Ä: {max_size}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:.1f}")
        print(f"  Noise: {n_noise}")
        print()
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å
    best = max(results, key=lambda r: r["coverage_percent"] - (r["max_cluster_size"] / 100))
    print(f"üéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–ê–†–ê–ú–ï–¢–†–´: {best}")
    return results

if __name__ == "__main__":
    test_hdbscan_params('sample_1000.csv')